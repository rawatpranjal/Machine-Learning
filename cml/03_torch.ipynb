{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Estimation with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Parameter Estimates ---\n",
      "   Param  True  Analytical MLE  PyTorch MLE  StatsModels MLE\n",
      "0     b1   1.0        0.978943     0.978944         0.978943\n",
      "1     b2   2.0        2.042813     2.042810         2.042813\n",
      "2  sigma   1.5        1.477383     1.477383         1.477382\n",
      "\n",
      "--- Hessians (flattened 3x3) ---\n",
      "                      Analytical      PyTorch  StatsModels\n",
      "b1_b1                 148.902165   148.902161   148.902338\n",
      "b1_b2                 113.320888   113.320892   113.321020\n",
      "b1_log_sigma            0.000000     0.000400     0.000150\n",
      "b2_b1                 113.320888   113.320892   113.321020\n",
      "b2_b2                 158.375468   158.375473   158.375654\n",
      "b2_log_sigma            0.000000     0.000776     0.000100\n",
      "log_sigma_b1            0.000000     0.000406     0.000150\n",
      "log_sigma_b2            0.000000     0.000776     0.000100\n",
      "log_sigma_log_sigma  1000.000000  1999.999878  2000.002320\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "# -----------------------------------------------------------------\n",
    "# 1. DATA SIMULATION: y = Xb + e, e ~ N(0, sigma^2)\n",
    "# -----------------------------------------------------------------\n",
    "np.random.seed(123)\n",
    "n_obs = 1000\n",
    "X_np = np.random.rand(n_obs, 2)  # 2 predictors, no intercept\n",
    "beta_true = np.array([1.0, 2.0])\n",
    "sigma_true = 1.5\n",
    "y_np = X_np @ beta_true + np.random.normal(0, sigma_true, n_obs)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2. ANALYTICAL (CLOSED-FORM) MLE\n",
    "#    beta_hat = (X'X)^(-1) X'y\n",
    "#    sigma_hat^2 = (1/n)*Sum(e^2), e = y - Xb\n",
    "# -----------------------------------------------------------------\n",
    "XX_inv = np.linalg.inv(X_np.T @ X_np)\n",
    "beta_analytic = XX_inv @ X_np.T @ y_np\n",
    "resid_analytic = y_np - X_np @ beta_analytic\n",
    "sigma_analytic = np.sqrt(np.mean(resid_analytic**2))\n",
    "\n",
    "# Hessian (3x3) for (b1, b2, log_sigma):\n",
    "#   Blocks:\n",
    "#     H_bb = (1/sigma^2)*(X'X), \n",
    "#     H_b_logSigma = 0 (cross-partials vanish if modeling log_sigma),\n",
    "#     H_logSigma_logSigma = n\n",
    "# Explanation:\n",
    "#   Negative log-likelihood for normal (in terms of log_sigma) is:\n",
    "#     NLL = 0.5*n*log(2π) + n*log_sigma + 0.5*(1/σ^2)*Sum((y - Xb)^2),\n",
    "#     with σ = exp(log_sigma).\n",
    "#   => partial^2 wrt b,b is (X'X)/σ^2\n",
    "#   => partial^2 wrt log_sigma, log_sigma is n\n",
    "#   => cross partial b <-> log_sigma = 0 for an OLS structure.\n",
    "# We evaluate at (beta_analytic, log_sigma_analytic).\n",
    "#   log_sigma_analytic = ln(sigma_analytic)\n",
    "H_bb = (X_np.T @ X_np) / (sigma_analytic**2)\n",
    "n = X_np.shape[0]\n",
    "H_logSigma_logSigma = n  # partial^2 wrt log_sigma\n",
    "H_b_logSigma = np.zeros((2,1))  # cross-partials\n",
    "H_analytic = np.block([\n",
    "    [H_bb,        H_b_logSigma],\n",
    "    [H_b_logSigma.T, H_logSigma_logSigma]\n",
    "])\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. PYTORCH MLE\n",
    "#    Parameterize as (b1, b2, log_sigma), minimize NLL\n",
    "# -----------------------------------------------------------------\n",
    "X_t = torch.tensor(X_np, dtype=torch.float32)\n",
    "y_t = torch.tensor(y_np, dtype=torch.float32)\n",
    "theta = torch.zeros(3, requires_grad=True)  # [b1, b2, log_sigma]\n",
    "\n",
    "def neg_loglik(theta, X, y):\n",
    "    b1, b2, log_sigma = theta\n",
    "    sigma = torch.exp(log_sigma)\n",
    "    y_hat = b1*X[:,0] + b2*X[:,1]\n",
    "    resid = y - y_hat\n",
    "    n = len(y)\n",
    "    return 0.5*n*np.log(2*np.pi) + n*log_sigma + 0.5*torch.sum(resid**2)/(sigma**2)\n",
    "\n",
    "optimizer = torch.optim.LBFGS([theta], lr=0.1)\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = neg_loglik(theta, X_t, y_t)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "for _ in range(50):\n",
    "    optimizer.step(closure)\n",
    "\n",
    "b1_torch, b2_torch, log_sigma_torch = theta.detach().numpy()\n",
    "sigma_torch = np.exp(log_sigma_torch)\n",
    "\n",
    "# Hessian from PyTorch (numerical)\n",
    "def torch_hessian(fn, param):\n",
    "    # param is a 1D torch tensor with requires_grad=True\n",
    "    # returns Hessian as a NumPy array\n",
    "    with torch.enable_grad():\n",
    "        grad_1 = torch.autograd.grad(fn(param), param, create_graph=True)[0]\n",
    "        dim = param.numel()\n",
    "        H = torch.zeros(dim, dim)\n",
    "        for i in range(dim):\n",
    "            grad_2 = torch.autograd.grad(grad_1[i], param, retain_graph=True)\n",
    "            H[i, :] = grad_2[0]\n",
    "        return H.detach().numpy()\n",
    "\n",
    "theta_t = theta.clone().requires_grad_(True)\n",
    "H_torch = torch_hessian(lambda p: neg_loglik(p, X_t, y_t), theta_t)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4. STATSMODELS MLE (GenericLikelihoodModel)\n",
    "# -----------------------------------------------------------------\n",
    "class NormalMLE(GenericLikelihoodModel):\n",
    "    def nloglikeobs(self, params):\n",
    "        b1, b2, log_sigma = params\n",
    "        sigma = np.exp(log_sigma)\n",
    "        y_hat = b1*self.exog[:,0] + b2*self.exog[:,1]\n",
    "        resid = self.endog - y_hat\n",
    "        return 0.5*np.log(2*np.pi) + log_sigma + 0.5*(resid**2)/(sigma**2)\n",
    "\n",
    "mle_mod = NormalMLE(endog=y_np, exog=X_np)\n",
    "mle_res = mle_mod.fit(start_params=[0,0,0], method=\"bfgs\", disp=False)\n",
    "b1_sm, b2_sm, log_sigma_sm = mle_res.params\n",
    "sigma_sm = np.exp(log_sigma_sm)\n",
    "\n",
    "# Hessian from StatsModels is the second derivative of *log-likelihood*, so we get\n",
    "# cov_params() from the *inverse* of the Hessian of the *negative* log-likelihood.\n",
    "H_statsmodels = mle_res.cov_params()\n",
    "# That is actually the covariance matrix. The Hessian can be approximated by inverse of that:\n",
    "H_statsmodels_inv = np.linalg.inv(H_statsmodels)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 5. TABLES\n",
    "#   - First: parameter estimates from True, Analytical, PyTorch, StatsModels\n",
    "#   - Second: Hessians from Analytical, PyTorch, StatsModels\n",
    "# -----------------------------------------------------------------\n",
    "# First table: parameter estimates\n",
    "df_estimates = pd.DataFrame({\n",
    "    \"Param\": [\"b1\", \"b2\", \"sigma\"],\n",
    "    \"True\": [beta_true[0], beta_true[1], sigma_true],\n",
    "    \"Analytical MLE\": [beta_analytic[0], beta_analytic[1], sigma_analytic],\n",
    "    \"PyTorch MLE\": [b1_torch, b2_torch, sigma_torch],\n",
    "    \"StatsModels MLE\": [b1_sm, b2_sm, sigma_sm]\n",
    "})\n",
    "\n",
    "# Second table: Hessians\n",
    "# - We have 3x3 from each approach (b1, b2, log_sigma)\n",
    "# - StatsModels gave us cov_params; invert to approximate Hessian\n",
    "labels = [\"b1\", \"b2\", \"log_sigma\"]\n",
    "\n",
    "# Flatten each Hessian in row-major order for quick comparison\n",
    "df_hessian = pd.DataFrame({\n",
    "    \"Analytical\": H_analytic.flatten(),\n",
    "    \"PyTorch\": H_torch.flatten(),\n",
    "    \"StatsModels\": H_statsmodels_inv.flatten()\n",
    "}, index=[\n",
    "    f\"{i}_{j}\" for i in labels for j in labels\n",
    "])\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# DISPLAY RESULTS\n",
    "# -----------------------------------------------------------------\n",
    "print(\"\\n--- Parameter Estimates ---\")\n",
    "print(df_estimates.round(6))\n",
    "\n",
    "print(\"\\n--- Hessians (flattened 3x3) ---\")\n",
    "print(df_hessian.round(6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Autograd\n",
    "\n",
    "- **Tensor Creation**: Define a scalar tensor $z$ with `requires_grad=True`.\n",
    "- **Function Definition**: Let $y = z^2 + z$.  \n",
    "  - Calling `y.backward()` computes $\\frac{dy}{dz}$ automatically.  \n",
    "  - The gradient is stored in `z.grad`.\n",
    "\n",
    "- **Weighted Mean**: Let $\\boldsymbol{x}$ be an $n$-dimensional vector, and $\\boldsymbol{w}$ be weights.  \n",
    "  - Compute $\\mu = \\frac{\\boldsymbol{x} \\boldsymbol{w}}{\\sum \\boldsymbol{w}}$.  \n",
    "  - Using `autograd.grad(mu, w)` in PyTorch gives $\\frac{\\partial \\mu}{\\partial \\boldsymbol{w}}$.  \n",
    "  - Scaling by $n$ ensures consistency with the factor used in the original R code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of z.grad: 2.0\n",
      "Weighted mean (mu): 0.5020911693572998\n",
      "Scaled gradient Lauto: [-0.36259884  0.38119125  0.14455152  0.28469217  0.29645973 -0.34025913\n",
      " -0.4394188   0.13489677 -0.16484235 -0.24163206 -0.3722259   0.35728598\n",
      "  0.06934097  0.33461872  0.05465974 -0.45357817 -0.28257996  0.3151117\n",
      " -0.31083164  0.41022095 -0.1823383  -0.07971539  0.15996979  0.2979676\n",
      "  0.26349908  0.3142938   0.00753673 -0.18019448 -0.2871688   0.457115\n",
      "  0.07162821  0.46687686 -0.00871816 -0.12694216  0.10923948  0.49101588\n",
      "  0.36262265 -0.09253472 -0.45909375 -0.0933989   0.25658783 -0.5003683\n",
      "  0.33057648 -0.24175839 -0.13594106  0.41859215 -0.1279648  -0.02390249\n",
      " -0.05818801 -0.13917899 -0.4206222  -0.07822332 -0.0089433  -0.2837852\n",
      "  0.32356018 -0.00751466 -0.26352262 -0.4017272   0.07573581 -0.02352423\n",
      "  0.21313336  0.4862398  -0.3016582   0.23106983 -0.07058103 -0.10677893\n",
      " -0.21312293 -0.14453895 -0.38423768 -0.04669796  0.08548484 -0.22254871\n",
      " -0.04841904 -0.03005252  0.15750834  0.08143689  0.4413944   0.02397001\n",
      "  0.1052564   0.02850937 -0.15699044 -0.33989242 -0.4857059  -0.2800337\n",
      "  0.05621258 -0.4746327  -0.18422471 -0.46109754  0.18447708  0.01136772\n",
      "  0.39021838  0.1275246   0.20189378  0.38013607  0.39452893 -0.13337363\n",
      "  0.08275132 -0.09854525  0.06430513  0.43910426]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. Simple demonstration of tensors and autodiff\n",
    "z = torch.tensor(0.5, requires_grad=True)  # analogous to .5 in R\n",
    "y = z**2 + z\n",
    "y.backward()  # differentiate y w.r.t. z\n",
    "print(\"Value of z.grad:\", z.grad.item())\n",
    "\n",
    "# 2. Weighted Mean Example\n",
    "torch.manual_seed(1234)\n",
    "n = 100\n",
    "\n",
    "# Generate data (like runif(n) in R)\n",
    "x0 = np.random.rand(n)\n",
    "x = torch.tensor(x0.reshape(1, -1), dtype=torch.float32)  # shape: (1, n)\n",
    "\n",
    "# Create weights (all ones), then convert to torch\n",
    "w0 = np.ones((n, 1), dtype=np.float32)\n",
    "w = torch.tensor(w0, requires_grad=True)\n",
    "\n",
    "# Weighted mean\n",
    "mu = (x @ w) / torch.sum(w)\n",
    "\n",
    "# Gradient of mu w.r.t. w\n",
    "grad = torch.autograd.grad(mu, w)\n",
    "\n",
    "# Multiply by n to mimic the R code's scaling\n",
    "Lauto = n * grad[0].detach().numpy()\n",
    "\n",
    "print(\"Weighted mean (mu):\", mu.item())\n",
    "print(\"Scaled gradient Lauto:\", Lauto.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
