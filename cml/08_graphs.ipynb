{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 404\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTrue ATE= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrue_ate\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 404\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 294\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# 1) Build full population => 2000\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m population\u001b[38;5;241m=\u001b[39m \u001b[43mbuild_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m true_ate\u001b[38;5;241m=\u001b[39m compute_true_ate(population)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue ATE in population= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrue_ate\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 64\u001b[0m, in \u001b[0;36mbuild_population\u001b[0;34m(n, n_nodes)\u001b[0m\n\u001b[1;32m     62\u001b[0m     p_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.3\u001b[39m)\n\u001b[1;32m     63\u001b[0m     G \u001b[38;5;241m=\u001b[39m generate_random_graph(n_nodes, p_val)\n\u001b[0;32m---> 64\u001b[0m     alpha_val, beta_val \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_alpha_beta_from_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     pop\u001b[38;5;241m.\u001b[39mappend((G, alpha_val, beta_val))\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pop\n",
      "Cell \u001b[0;32mIn[11], line 47\u001b[0m, in \u001b[0;36mcompute_alpha_beta_from_graph\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(nodes)):\n\u001b[1;32m     46\u001b[0m                 sub \u001b[38;5;241m=\u001b[39m G\u001b[38;5;241m.\u001b[39msubgraph([nodes[i], nodes[j], nodes[k], nodes[l]])\n\u001b[0;32m---> 47\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumber_of_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m     48\u001b[0m                     four_cycles \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     49\u001b[0m alpha_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(total_tris \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mfour_cycles)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/networkx/classes/graph.py:1959\u001b[0m, in \u001b[0;36mGraph.number_of_edges\u001b[0;34m(self, u, v)\u001b[0m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of edges between two nodes.\u001b[39;00m\n\u001b[1;32m   1914\u001b[0m \n\u001b[1;32m   1915\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1956\u001b[0m \n\u001b[1;32m   1957\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m u \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adj[u]:\n\u001b[1;32m   1961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/networkx/classes/graph.py:1905\u001b[0m, in \u001b[0;36mGraph.size\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msize\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of edges or total of all edge weights.\u001b[39;00m\n\u001b[1;32m   1871\u001b[0m \n\u001b[1;32m   1872\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1903\u001b[0m \u001b[38;5;124;03m    6.0\u001b[39;00m\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1905\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(d \u001b[38;5;28;01mfor\u001b[39;00m v, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m(weight\u001b[38;5;241m=\u001b[39mweight))\n\u001b[1;32m   1906\u001b[0m     \u001b[38;5;66;03m# If `weight` is None, the sum of the degrees is guaranteed to be\u001b[39;00m\n\u001b[1;32m   1907\u001b[0m     \u001b[38;5;66;03m# even, so we can perform integer division and hence return an\u001b[39;00m\n\u001b[1;32m   1908\u001b[0m     \u001b[38;5;66;03m# integer. Otherwise, the sum of the weighted degrees is not\u001b[39;00m\n\u001b[1;32m   1909\u001b[0m     \u001b[38;5;66;03m# guaranteed to be an integer, so we perform \"real\" division.\u001b[39;00m\n\u001b[1;32m   1910\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m s \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/functools.py:969\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    967\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 969\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    971\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/networkx/classes/graph.py:1517\u001b[0m, in \u001b[0;36mGraph.degree\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdegree\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A DegreeView for the Graph as G.degree or G.degree().\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m \n\u001b[1;32m   1485\u001b[0m \u001b[38;5;124;03m    The node degree is the number of edges adjacent to the node.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;124;03m    [(0, 1), (1, 2), (2, 2)]\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDegreeView\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/networkx/classes/reportviews.py:418\u001b[0m, in \u001b[0;36mDiDegreeView.__init__\u001b[0;34m(self, G, nbunch, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, G, nbunch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m \u001b[38;5;241m=\u001b[39m G\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_succ \u001b[38;5;241m=\u001b[39m G\u001b[38;5;241m.\u001b[39m_succ \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(G, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_succ\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m G\u001b[38;5;241m.\u001b[39m_adj\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pred \u001b[38;5;241m=\u001b[39m G\u001b[38;5;241m.\u001b[39m_pred \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(G, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m G\u001b[38;5;241m.\u001b[39m_adj\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "# Attempt imports for econml + LightGBM\n",
    "try:\n",
    "    from econml.dml import CausalForestDML\n",
    "    from lightgbm import LGBMRegressor\n",
    "    has_econml = True\n",
    "except ImportError:\n",
    "    has_econml = False\n",
    "    print(\"econml or lightgbm not installed. For CF, run: pip install econml lightgbm\")\n",
    "\n",
    "############################################################################\n",
    "# 1) DATA GENERATION\n",
    "############################################################################\n",
    "\n",
    "def generate_random_graph(n_nodes=10, p=0.2):\n",
    "    \"\"\"Generate an Erdos-Renyi G(n,p) random graph with n_nodes.\"\"\"\n",
    "    return nx.erdos_renyi_graph(n_nodes, p)\n",
    "\n",
    "def compute_alpha_beta_from_graph(G):\n",
    "    \"\"\"\n",
    "    alpha = (# of triangles) + 2*(# of 4-cycles).\n",
    "    beta  = 1 + 0.05*(avg_deg) - 0.01*(num_edges).\n",
    "    \"\"\"\n",
    "    # count triangles\n",
    "    tri_count = nx.triangles(G)  # dict: node->#triangles\n",
    "    total_tris = sum(tri_count.values()) // 3\n",
    "\n",
    "    # count 4-cycles\n",
    "    nodes = list(G.nodes())\n",
    "    four_cycles = 0\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            for k in range(j+1, len(nodes)):\n",
    "                for l in range(k+1, len(nodes)):\n",
    "                    sub = G.subgraph([nodes[i], nodes[j], nodes[k], nodes[l]])\n",
    "                    if sub.number_of_edges() == 4:\n",
    "                        four_cycles += 1\n",
    "    alpha_val = float(total_tris + 2*four_cycles)\n",
    "\n",
    "    # simpler structural stats -> beta\n",
    "    num_edges = G.number_of_edges()\n",
    "    avg_deg   = 2.0*num_edges / max(G.number_of_nodes(),1)\n",
    "    beta_val  = 1.0 + 0.05*avg_deg - 0.01*num_edges\n",
    "\n",
    "    return alpha_val, beta_val\n",
    "\n",
    "def build_population(n=2000, n_nodes=10):\n",
    "    \"\"\"Build a list of random graphs + (alpha,beta).\"\"\"\n",
    "    pop = []\n",
    "    for _ in range(n):\n",
    "        p_val = np.random.uniform(0.1,0.3)\n",
    "        G = generate_random_graph(n_nodes, p_val)\n",
    "        alpha_val, beta_val = compute_alpha_beta_from_graph(G)\n",
    "        pop.append((G, alpha_val, beta_val))\n",
    "    return pop\n",
    "\n",
    "def compute_true_ate(pop):\n",
    "    return np.mean([p[2] for p in pop])\n",
    "\n",
    "def sample_dataset(population, sample_size=1000):\n",
    "    \"\"\"\n",
    "    For each G, create T=Bernoulli(0.5), Y=alpha+beta*T+noise.\n",
    "    Return list of (G,w,y,alpha,beta).\n",
    "    \"\"\"\n",
    "    idxs = np.random.choice(len(population), size=sample_size, replace=False)\n",
    "    data = []\n",
    "    for idx in idxs:\n",
    "        G, alpha_val, beta_val = population[idx]\n",
    "        w = np.random.binomial(1, 0.5)\n",
    "        noise = np.random.randn()\n",
    "        y = alpha_val + beta_val*w + noise\n",
    "        data.append((G, w, y, alpha_val, beta_val))\n",
    "    return data\n",
    "\n",
    "############################################################################\n",
    "# 2) COLLATE + DATASET\n",
    "############################################################################\n",
    "\n",
    "class GraphData(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "def adjacency_to_flat(G):\n",
    "    \"\"\"Flatten adjacency => 10x10=100D for 10-node graphs.\"\"\"\n",
    "    A = nx.to_numpy_array(G, nodelist=sorted(G.nodes()))\n",
    "    return A.reshape(-1).astype(np.float32)\n",
    "\n",
    "def adjacency_to_edge_index(G):\n",
    "    \"\"\"Return (node_features, edge_idx) for PyG GraphConv usage.\"\"\"\n",
    "    n = G.number_of_nodes()\n",
    "    edges = list(G.edges())\n",
    "    src = [e[0] for e in edges]\n",
    "    dst = [e[1] for e in edges]\n",
    "    # make undirected\n",
    "    src_rev = src + dst\n",
    "    dst_rev = dst + src\n",
    "    x_feat = torch.eye(n, dtype=torch.float32)\n",
    "    edge_idx = torch.tensor([src_rev, dst_rev], dtype=torch.long)\n",
    "    return x_feat, edge_idx\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    batch => list of (G,w,y,alpha,beta)\n",
    "    We'll return (G_list, W_t, Y_t, A_t, B_t).\n",
    "    \"\"\"\n",
    "    G_list, W_list, Y_list, A_list, B_list = [],[],[],[],[]\n",
    "    for (G,w,y,a,b) in batch:\n",
    "        G_list.append(G)\n",
    "        W_list.append(w)\n",
    "        Y_list.append(y)\n",
    "        A_list.append(a)\n",
    "        B_list.append(b)\n",
    "    W_t = torch.tensor(W_list, dtype=torch.float32).view(-1,1)\n",
    "    Y_t = torch.tensor(Y_list, dtype=torch.float32).view(-1,1)\n",
    "    A_t = torch.tensor(A_list, dtype=torch.float32)\n",
    "    B_t = torch.tensor(B_list, dtype=torch.float32)\n",
    "    return (G_list, W_t, Y_t, A_t, B_t)\n",
    "\n",
    "############################################################################\n",
    "# 3) MODELS: MLP + GNN\n",
    "############################################################################\n",
    "\n",
    "class MLPAlphaBeta(nn.Module):\n",
    "    \"\"\"Flatten adjacency => MLP => (alpha,beta).\"\"\"\n",
    "    def __init__(self, input_dim=100, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "    def forward(self, G_list):\n",
    "        # flatten adjacency for each graph\n",
    "        vecs = []\n",
    "        for G in G_list:\n",
    "            x = adjacency_to_flat(G)\n",
    "            vecs.append(x)\n",
    "        X_t = torch.stack([torch.from_numpy(v) for v in vecs], dim=0)  # (batch,100)\n",
    "        ab_pred = self.net(X_t)  # (batch,2)\n",
    "        return ab_pred\n",
    "\n",
    "class GCNAlphaBeta(nn.Module):\n",
    "    \"\"\"A simple 2-layer GraphConv => (alpha,beta).\"\"\"\n",
    "    def __init__(self, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = pyg_nn.GraphConv(in_channels=10, out_channels=hidden_dim)\n",
    "        self.conv2 = pyg_nn.GraphConv(in_channels=hidden_dim, out_channels=hidden_dim)\n",
    "        self.head  = nn.Linear(hidden_dim, 2)\n",
    "    def forward(self, G_list):\n",
    "        outs = []\n",
    "        for G in G_list:\n",
    "            x_feat, edge_idx = adjacency_to_edge_index(G)\n",
    "            edge_idx = to_undirected(edge_idx)\n",
    "            h = self.conv1(x_feat, edge_idx)\n",
    "            h = torch.relu(h)\n",
    "            h = self.conv2(h, edge_idx)\n",
    "            h = torch.relu(h)\n",
    "            out_graph = h.mean(dim=0)  # global mean pool => shape (hidden_dim,)\n",
    "            outs.append(out_graph)\n",
    "        X_t = torch.stack(outs, dim=0)  # (batch, hidden_dim)\n",
    "        ab_pred = self.head(X_t)       # (batch,2)\n",
    "        return ab_pred\n",
    "\n",
    "############################################################################\n",
    "# 4) TRAIN + EVALUATION\n",
    "############################################################################\n",
    "\n",
    "def dr_ate(alpha_hat, beta_hat, w_arr, y_arr):\n",
    "    \"\"\"Double-robust style ATE with p=0.5.\"\"\"\n",
    "    mu0 = alpha_hat\n",
    "    mu1 = alpha_hat + beta_hat\n",
    "    e   = 0.5\n",
    "    IF  = (mu1 + w_arr*(y_arr - mu1)/e) - (mu0 + (1 - w_arr)*(y_arr - mu0)/(1-e))\n",
    "    ate = IF.mean()\n",
    "    se  = IF.std(ddof=1)/np.sqrt(len(IF))\n",
    "    return ate, se\n",
    "\n",
    "def rmse(true_vals, pred_vals):\n",
    "    return np.sqrt(np.mean((true_vals - pred_vals)**2))\n",
    "\n",
    "def r2_score(true_vals, pred_vals):\n",
    "    true_vals = np.array(true_vals)\n",
    "    pred_vals = np.array(pred_vals)\n",
    "    sse = np.sum((true_vals - pred_vals)**2)\n",
    "    sst = np.sum((true_vals - np.mean(true_vals))**2)\n",
    "    return 1.0 - sse/sst if sst>1e-8 else 0.0\n",
    "\n",
    "def evaluate_model(model, loader, device='cpu', model_type=\"MLP\"):\n",
    "    \"\"\"Compute (ate,se, RMSE(y), R2(y), RMSE(a), R2(a), RMSE(b), R2(b)).\"\"\"\n",
    "    alphaP, betaP = [], []\n",
    "    alphaT, betaT= [], []\n",
    "    wA, yA, yPred= [], [], []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for (G_list, W_t, Y_t, A_t, B_t) in loader:\n",
    "            ab = model(G_list)  # => (batch,2)\n",
    "            for i in range(len(G_list)):\n",
    "                a_hat= ab[i,0].item()\n",
    "                b_hat= ab[i,1].item()\n",
    "                alphaP.append(a_hat)\n",
    "                betaP.append(b_hat)\n",
    "            alphaT.extend(A_t.tolist())\n",
    "            betaT.extend(B_t.tolist())\n",
    "            wA.extend(W_t.view(-1).tolist())\n",
    "            yA.extend(Y_t.view(-1).tolist())\n",
    "            # predicted y\n",
    "            for i in range(len(G_list)):\n",
    "                yPred.append(ab[i,0].item() + ab[i,1].item()*W_t[i].item())\n",
    "    # Convert\n",
    "    alphaP= np.array(alphaP)\n",
    "    betaP=  np.array(betaP)\n",
    "    alphaT= np.array(alphaT)\n",
    "    betaT=  np.array(betaT)\n",
    "    wA   = np.array(wA)\n",
    "    yA   = np.array(yA)\n",
    "    yPred= np.array(yPred)\n",
    "\n",
    "    # ATE, SE\n",
    "    ate, se= dr_ate(alphaP, betaP, wA, yA)\n",
    "    # RMSE + R2\n",
    "    rmse_y= rmse(yA, yPred)\n",
    "    r2y   = r2_score(yA, yPred)\n",
    "    rmse_a= rmse(alphaT, alphaP)\n",
    "    r2a   = r2_score(alphaT, alphaP)\n",
    "    rmse_b= rmse(betaT, betaP)\n",
    "    r2b   = r2_score(betaT, betaP)\n",
    "    return (ate,se, rmse_y, r2y, rmse_a, r2a, rmse_b, r2b)\n",
    "\n",
    "def train_one_epoch(model, loader, opt, device='cpu'):\n",
    "    model.train()\n",
    "    mse= nn.MSELoss()\n",
    "    for (G_list, W_t, Y_t, A_t, B_t) in loader:\n",
    "        ab_pred= model(G_list)\n",
    "        alpha_pred= ab_pred[:,0:1]\n",
    "        beta_pred= ab_pred[:,1:2]\n",
    "        Y_pred= alpha_pred + beta_pred*W_t\n",
    "        loss= mse(Y_pred, Y_t)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "############################################################################\n",
    "# 5) CAUSAL FOREST + UTILS\n",
    "############################################################################\n",
    "\n",
    "if has_econml:\n",
    "    from econml.dml import CausalForestDML\n",
    "    from lightgbm import LGBMRegressor\n",
    "\n",
    "def prepare_cf_data(samples):\n",
    "    \"\"\"\n",
    "    CF needs X, W, Y + also the true beta to measure error.\n",
    "    We'll flatten adjacency => X, store W,y,beta\n",
    "    \"\"\"\n",
    "    X_list, W_list, Y_list, B_list= [],[],[],[]\n",
    "    for (G,w,y,a,b) in samples:\n",
    "        A= adjacency_to_flat(G)\n",
    "        X_list.append(A)\n",
    "        W_list.append(w)\n",
    "        Y_list.append(y)\n",
    "        B_list.append(b)\n",
    "    X_np= np.array(X_list,dtype=np.float32)\n",
    "    W_np= np.array(W_list,dtype=np.float32)\n",
    "    Y_np= np.array(Y_list,dtype=np.float32)\n",
    "    B_np= np.array(B_list,dtype=np.float32)\n",
    "    return X_np, W_np, Y_np, B_np\n",
    "\n",
    "############################################################################\n",
    "# 6) MAIN\n",
    "############################################################################\n",
    "\n",
    "def main():\n",
    "    import math\n",
    "    import random\n",
    "\n",
    "    # 1) Build full population => 2000\n",
    "    population= build_population(n=2000, n_nodes=10)\n",
    "    true_ate= compute_true_ate(population)\n",
    "    print(f\"True ATE in population= {true_ate:.4f}\")\n",
    "\n",
    "    # We'll define train_sizes, sample_test\n",
    "    train_sizes= [100, 500, 1000, 1500]\n",
    "    sample_test= 500\n",
    "\n",
    "    # We'll sample test_data from the same population\n",
    "    test_data= sample_dataset(population, sample_size=sample_test)\n",
    "    # We'll define a test_loader for evaluation\n",
    "    test_ds= GraphData(test_data)\n",
    "    test_loader= DataLoader(test_ds, batch_size=64, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "    # We'll store rows => (N, Model, ATE, SE, RMSE(y), R2(y), RMSE(a), R2(a), RMSE(b), R2(b))\n",
    "    results= []\n",
    "\n",
    "    # Hyperparams\n",
    "    mlp_epochs= 500\n",
    "    gnn_epochs= 500\n",
    "    lr=1e-3\n",
    "\n",
    "    def train_mlp(train_samples, epochs):\n",
    "        ds= GraphData(train_samples)\n",
    "        dl= DataLoader(ds, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "        model= MLPAlphaBeta(input_dim=100, hidden_dim=64)\n",
    "        opt= optim.Adam(model.parameters(), lr=lr)\n",
    "        for ep in tqdm(range(epochs), desc=f\"MLP({len(train_samples)})\"):\n",
    "            train_one_epoch(model, dl, opt)\n",
    "        return model\n",
    "\n",
    "    def train_gnn(train_samples, epochs):\n",
    "        ds= GraphData(train_samples)\n",
    "        dl= DataLoader(ds, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "        model= GCNAlphaBeta(hidden_dim=32)\n",
    "        opt= optim.Adam(model.parameters(), lr=lr)\n",
    "        for ep in tqdm(range(epochs), desc=f\"GNN({len(train_samples)})\"):\n",
    "            train_one_epoch(model, dl, opt)\n",
    "        return model\n",
    "\n",
    "    for N in train_sizes:\n",
    "        # sample new train_data from population (be sure not to overlap test_data)\n",
    "        train_data= sample_dataset(population, sample_size=N)\n",
    "\n",
    "        # (A) MLP\n",
    "        mlp_model= train_mlp(train_data, mlp_epochs)\n",
    "        mlp_res= evaluate_model(mlp_model, test_loader, model_type=\"MLP\")\n",
    "\n",
    "        # (B) GNN\n",
    "        gnn_model= train_gnn(train_data, gnn_epochs)\n",
    "        gnn_res= evaluate_model(gnn_model, test_loader, model_type=\"GNN\")\n",
    "\n",
    "        # (C) CF if installed\n",
    "        if has_econml:\n",
    "            X_cf_train, W_cf_train, Y_cf_train, B_cf_train= prepare_cf_data(train_data)\n",
    "            X_cf_test,  W_cf_test,  Y_cf_test,  B_cf_test=  prepare_cf_data(test_data)\n",
    "\n",
    "            cf_model= CausalForestDML(\n",
    "                model_y=LGBMRegressor(verbose=-1),\n",
    "                model_t=LGBMRegressor(verbose=-1),\n",
    "                n_estimators=400,\n",
    "                min_samples_leaf=10,\n",
    "                max_depth=25,\n",
    "                random_state=42\n",
    "            )\n",
    "            cf_model.fit(Y_cf_train, W_cf_train, X=X_cf_train)\n",
    "\n",
    "            b_hat_cf= cf_model.effect(X_cf_test) # shape=(test_size,)\n",
    "            ate_cf= np.mean(b_hat_cf)\n",
    "            lb_cf, ub_cf= cf_model.effect_interval(X_cf_test)\n",
    "            se_cf= (ub_cf - lb_cf).mean()/(2*1.96)\n",
    "            # measure RMSE(b) & R2(b)\n",
    "            rb_cf= rmse(B_cf_test, b_hat_cf)\n",
    "            r2b_cf= r2_score(B_cf_test, b_hat_cf)\n",
    "\n",
    "            # CF doesn't produce alpha or Y => placeholders\n",
    "            cf_res= (ate_cf, se_cf, None, None, None, None, rb_cf, r2b_cf)\n",
    "        else:\n",
    "            cf_res= (np.nan, np.nan, None, None, None, None, None, None)\n",
    "\n",
    "        # Save\n",
    "        results.append( (N,\"MLP\", *mlp_res) )\n",
    "        results.append( (N,\"GNN\", *gnn_res) )\n",
    "        results.append( (N,\"CF\",  *cf_res) )\n",
    "\n",
    "    print(\"\\n=== RESULTS TABLE ===\")\n",
    "    print(\"DataN | Model |  ATE_est |  ATE_se |  RMSE(y) | R2(y)   | RMSE(a) | R2(a)   | RMSE(b) | R2(b)\")\n",
    "    for row in results:\n",
    "        (N,model,ate,se,ry,r2y,ra,r2a,rb,r2b)= row\n",
    "        if model==\"CF\":\n",
    "            # CF => no Y, alpha\n",
    "            ate_s= f\"{ate:8.4f}\" if ate is not None else \"  --\"\n",
    "            se_s= f\"{se:8.4f}\" if se is not None else \"  --\"\n",
    "            rb_s= f\"{rb:7.4f}\" if rb is not None else \"  --\"\n",
    "            r2b_s= f\"{r2b:7.4f}\" if r2b is not None else \"  --\"\n",
    "            print(f\"{N:<5} {model:<3} | {ate_s} | {se_s} |     --   |   --   |   --    |   --   | {rb_s} | {r2b_s}\")\n",
    "        else:\n",
    "            ate_s= f\"{ate:8.4f}\"\n",
    "            se_s= f\"{se:8.4f}\"\n",
    "            ry_s= f\"{ry:7.4f}\"\n",
    "            r2y_s=f\"{r2y:7.4f}\"\n",
    "            ra_s= f\"{ra:7.4f}\"\n",
    "            r2a_s=f\"{r2a:7.4f}\"\n",
    "            rb_s= f\"{rb:7.4f}\"\n",
    "            r2b_s=f\"{r2b:7.4f}\"\n",
    "            print(f\"{N:<5} {model:<3} | {ate_s} | {se_s} | {ry_s} | {r2y_s} | {ra_s} | {r2a_s} | {rb_s} | {r2b_s}\")\n",
    "\n",
    "    print(f\"\\nTrue ATE= {true_ate:.4f}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
