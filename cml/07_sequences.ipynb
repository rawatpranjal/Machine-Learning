{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterogeneous Treatment Effects with Sequential Paths\n",
    "\n",
    "**Data Generation**  \n",
    "- Paths: Each $X$ is a sequence of length 5–30, with actions in $\\{0, 1, 2, 3\\}$.  \n",
    "- Alpha, Beta:  \n",
    "  $\\alpha(X) = \\sum_{t=1}^{T-1} \\big[\\mathbf{1}\\{X_t=1, X_{t+1}=3\\} \\cdot 5 + \\mathbf{1}\\{X_t=2, X_{t+1}=2\\} \\cdot 10 \\big] + 0.2 \\, T$,  \n",
    "  $\\beta(X) = 5 - 0.2 \\, (\\#\\text{Up}) - 0.1 \\, T$.  \n",
    "- Outcome:  \n",
    "  $Y = \\alpha(X) + \\beta(X)\\,W + \\varepsilon$,  \n",
    "  $W \\sim \\text{Bernoulli}(0.5)$,  \n",
    "  $\\varepsilon \\sim \\mathcal{N}(0,1)$.\n",
    "\n",
    "**Models**  \n",
    "1. DNN: Flatten the path into a one-hot encoding, input into an MLP predicting $\\alpha$ and $\\beta$.  \n",
    "2. LSTM: Embed the sequence, process it with an LSTM, output $\\alpha$ and $\\beta$.  \n",
    "3. Transformer: Embed using positional encoding, apply self-attention, output $\\alpha$ and $\\beta$.\n",
    "\n",
    "**Double-Robust ATE**  \n",
    "$$\n",
    "\\widehat{\\text{ATE}}\n",
    "=\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} \\biggl[\\bigl(\\hat{\\alpha}_i + \\hat{\\beta}_i\\bigr) \n",
    "+ \\frac{W_i}{p} \\bigl(Y_i - (\\hat{\\alpha}_i + \\hat{\\beta}_i)\\bigr) \n",
    "- \\hat{\\alpha}_i \n",
    "- \\frac{1-W_i}{1-p} \\bigl(Y_i - \\hat{\\alpha}_i\\bigr)\\biggr], \n",
    "\\quad p=0.5.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population => alpha,beta in [0,1], True ATE= 0.6413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DNN: 100%|██████████| 100/100 [00:13<00:00,  7.43it/s]\n",
      "LSTM: 100%|██████████| 150/150 [00:30<00:00,  4.85it/s]\n",
      "Transformer: 100%|██████████| 50/50 [00:28<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting CausalForestDML with more leaves...\n",
      "\n",
      "=== TRAIN vs. TEST RESULTS ===\n",
      "Model        |    ATE(train)  R^2(y)   R^2(a)   R^2(b)   ||   ATE(test)   R^2(y)   R^2(a)   R^2(b)\n",
      "DNN          |     0.6205    0.357   -8.097   -2.518  ||     0.7189   -0.128   -9.177   -2.374\n",
      "LSTM         |     0.6271    0.103   -0.090    0.370  ||     0.6957    0.105   -0.143    0.372\n",
      "Transformer  |     0.6297    0.096   -0.015    0.721  ||     0.7007    0.113   -0.020    0.725\n",
      "CForest      |     0.6198      N/A      N/A    0.505  ||     0.6193      N/A      N/A    0.550\n",
      "\n",
      "True ATE= 0.6413\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from econml.dml import CausalForestDML\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "############################################################################\n",
    "# 0) HYPERPARAMETERS\n",
    "############################################################################\n",
    "\n",
    "# General / data / training\n",
    "GEN_H = {\n",
    "    \"max_len\":      30,\n",
    "    \"action_dim\":   4,\n",
    "    \"pop_size\":     20000,\n",
    "    \"sample_train\": 10000,\n",
    "    \"sample_test\":  2000,\n",
    "    \"batch_size\":   256,\n",
    "    \"lr\":           1e-3\n",
    "}\n",
    "\n",
    "# Neural net\n",
    "NN_H = {\n",
    "    \"dnn_hidden_dim\":  16,\n",
    "    \"dnn_num_layers\":  4,\n",
    "    \"dnn_epochs\":      100,   # smaller number for demonstration\n",
    "\n",
    "    \"lstm_embed_dim\":  4,\n",
    "    \"lstm_hidden_dim\": 8,\n",
    "    \"lstm_epochs\":     150,\n",
    "\n",
    "    \"tf_d_model\":      4,\n",
    "    \"tf_nhead\":        1,\n",
    "    \"tf_num_layers\":   1,\n",
    "    \"tf_epochs\":       50\n",
    "}\n",
    "\n",
    "# Causal Forest\n",
    "CF_H = {\n",
    "    \"n_estimators\":  200,\n",
    "    \"max_depth\":     None,\n",
    "    \"num_leaves\":    31,\n",
    "    \"criterion\":     \"mse\"\n",
    "}\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# 1) DATA GENERATION\n",
    "############################################################################\n",
    "\n",
    "def generate_random_path(max_len=30, action_dim=4):\n",
    "    length = np.random.randint(5, max_len+1)\n",
    "    return np.random.randint(0, action_dim, size=length)\n",
    "\n",
    "def compute_alpha_beta_from_path(path):\n",
    "    length = len(path)\n",
    "    alpha_val = 0.0\n",
    "    for t in range(length-1):\n",
    "        if path[t] == 1 and path[t+1] == 3:\n",
    "            alpha_val += 5.0\n",
    "        if path[t] == 2 and path[t+1] == 2:\n",
    "            alpha_val += 10.0\n",
    "    alpha_val += 0.2 * length\n",
    "    n_up = np.sum(path==0)\n",
    "    beta_val = 5.0 - 0.2*n_up - 0.1*length\n",
    "    return alpha_val, beta_val\n",
    "\n",
    "def build_population(n=20000, max_len=30, action_dim=4):\n",
    "    pop = []\n",
    "    for _ in range(n):\n",
    "        path = generate_random_path(max_len, action_dim)\n",
    "        a, b = compute_alpha_beta_from_path(path)\n",
    "        pop.append((path, a, b))\n",
    "    return pop\n",
    "\n",
    "def minmax_normalize_population(pop):\n",
    "    alpha_vals = [p[1] for p in pop]\n",
    "    beta_vals  = [p[2] for p in pop]\n",
    "    a_min, a_max = min(alpha_vals), max(alpha_vals)\n",
    "    b_min, b_max = min(beta_vals),  max(beta_vals)\n",
    "    \n",
    "    new_pop = []\n",
    "    for (path, a, b) in pop:\n",
    "        a_norm = (a - a_min)/(a_max - a_min + 1e-9)\n",
    "        b_norm = (b - b_min)/(b_max - b_min + 1e-9)\n",
    "        new_pop.append((path, a_norm, b_norm))\n",
    "    return new_pop, (a_min, a_max, b_min, b_max)\n",
    "\n",
    "def compute_true_ate(pop):\n",
    "    return np.mean([p[2] for p in pop])\n",
    "\n",
    "def sample_dataset(pop, sample_size=10000, max_len=30, action_dim=4):\n",
    "    idxs = np.random.choice(len(pop), size=sample_size, replace=False)\n",
    "    data = []\n",
    "    for idx in idxs:\n",
    "        path, a, b = pop[idx]\n",
    "        w = np.random.binomial(1, 0.5)\n",
    "        noise = np.random.randn()\n",
    "        y = a + b*w + noise\n",
    "        data.append((path, w, y, a, b))\n",
    "    return data\n",
    "\n",
    "############################################################################\n",
    "# 2) MODEL DEFS\n",
    "############################################################################\n",
    "\n",
    "def path_to_onehot(path, max_len=30, action_dim=4):\n",
    "    arr = np.zeros((max_len, action_dim), dtype=np.float32)\n",
    "    length = min(len(path), max_len)\n",
    "    for i in range(length):\n",
    "        arr[i, path[i]] = 1.0\n",
    "    return arr.reshape(-1)\n",
    "\n",
    "def collate_fn_dnn(samples, max_len=30, action_dim=4):\n",
    "    X_list, W_list, Y_list, A_list, B_list = [],[],[],[],[]\n",
    "    for path,w,y,a,b in samples:\n",
    "        oh = path_to_onehot(path, max_len, action_dim)\n",
    "        X_list.append(oh)\n",
    "        W_list.append(w)\n",
    "        Y_list.append(y)\n",
    "        A_list.append(a)\n",
    "        B_list.append(b)\n",
    "    X_t = torch.tensor(X_list, dtype=torch.float32)\n",
    "    W_t = torch.tensor(W_list, dtype=torch.float32).view(-1,1)\n",
    "    Y_t = torch.tensor(Y_list, dtype=torch.float32).view(-1,1)\n",
    "    A_t = torch.tensor(A_list, dtype=torch.float32)\n",
    "    B_t = torch.tensor(B_list, dtype=torch.float32)\n",
    "    return X_t, W_t, Y_t, A_t, B_t\n",
    "\n",
    "class DNNAlphaBeta(nn.Module):\n",
    "    \"\"\"\n",
    "    Deeper feedforward => (alpha, beta).\n",
    "    'num_layers': how many hidden layers\n",
    "    'hidden_dim': dimension of each hidden layer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=120, hidden_dim=16, num_layers=3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        # first hidden layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        # additional hidden layers\n",
    "        for _ in range(num_layers-1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        # final\n",
    "        layers.append(nn.Linear(hidden_dim, 2))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def collate_fn_lstm(samples, max_len=30):\n",
    "    x_padded, seq_len = [], []\n",
    "    W_list, Y_list, A_list, B_list = [],[],[],[]\n",
    "    for path,w,y,a,b in samples:\n",
    "        arr = np.zeros(max_len, dtype=np.int64)\n",
    "        plen = min(len(path), max_len)\n",
    "        arr[:plen] = path[:plen]\n",
    "        x_padded.append(arr)\n",
    "        seq_len.append(plen)\n",
    "        W_list.append(w)\n",
    "        Y_list.append(y)\n",
    "        A_list.append(a)\n",
    "        B_list.append(b)\n",
    "    x_pad = torch.tensor(x_padded, dtype=torch.long)\n",
    "    slen  = torch.tensor(seq_len,  dtype=torch.long)\n",
    "    W_t   = torch.tensor(W_list, dtype=torch.float32).view(-1,1)\n",
    "    Y_t   = torch.tensor(Y_list, dtype=torch.float32).view(-1,1)\n",
    "    A_t   = torch.tensor(A_list, dtype=torch.float32)\n",
    "    B_t   = torch.tensor(B_list, dtype=torch.float32)\n",
    "    return (x_pad, slen, W_t, Y_t, A_t, B_t)\n",
    "\n",
    "class LSTMAlphaBeta(nn.Module):\n",
    "    def __init__(self, action_dim=4, embed_dim=4, hidden_dim=4):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(action_dim, embed_dim)\n",
    "        self.lstm  = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.head  = nn.Linear(hidden_dim, 2)\n",
    "    def forward(self, x_padded, seq_len):\n",
    "        emb = self.embed(x_padded)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            emb, seq_len, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        return self.head(h_n[-1])\n",
    "\n",
    "def collate_fn_transformer(samples, max_len=30):\n",
    "    x_padded, seq_len = [], []\n",
    "    W_list, Y_list, A_list, B_list = [],[],[],[]\n",
    "    for path,w,y,a,b in samples:\n",
    "        arr = np.zeros(max_len, dtype=np.int64)\n",
    "        plen = min(len(path), max_len)\n",
    "        arr[:plen] = path[:plen]\n",
    "        x_padded.append(arr)\n",
    "        seq_len.append(plen)\n",
    "        W_list.append(w)\n",
    "        Y_list.append(y)\n",
    "        A_list.append(a)\n",
    "        B_list.append(b)\n",
    "    x_pad = torch.tensor(x_padded, dtype=torch.long)\n",
    "    slen  = torch.tensor(seq_len,  dtype=torch.long)\n",
    "    W_t   = torch.tensor(W_list, dtype=torch.float32).view(-1,1)\n",
    "    Y_t   = torch.tensor(Y_list, dtype=torch.float32).view(-1,1)\n",
    "    A_t   = torch.tensor(A_list, dtype=torch.float32)\n",
    "    B_t   = torch.tensor(B_list, dtype=torch.float32)\n",
    "    return (x_pad, slen, W_t, Y_t, A_t, B_t)\n",
    "\n",
    "class TransformerAlphaBeta(nn.Module):\n",
    "    def __init__(self, action_dim=4, d_model=8, nhead=1, num_layers=1, max_len=30):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(action_dim, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=16, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.head = nn.Linear(d_model, 2)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, x_padded, seq_len):\n",
    "        B, L = x_padded.shape\n",
    "        emb = self.embed(x_padded) + self.pos_embed[:, :L, :]\n",
    "        mask = torch.zeros((B, L), dtype=torch.bool, device=emb.device)\n",
    "        for i in range(B):\n",
    "            if seq_len[i]<L:\n",
    "                mask[i, seq_len[i]:] = True\n",
    "        out_seq = self.transformer(emb, src_key_padding_mask=mask)\n",
    "        out_vec = []\n",
    "        for i in range(B):\n",
    "            plen = seq_len[i].item()\n",
    "            if plen>0:\n",
    "                out_vec.append(out_seq[i,:plen,:].mean(dim=0))\n",
    "            else:\n",
    "                out_vec.append(torch.zeros(d_model, device=out_seq.device))\n",
    "        out_vec = torch.stack(out_vec, dim=0)\n",
    "        return self.head(out_vec)\n",
    "\n",
    "############################################################################\n",
    "# 3) METRICS + EVALUATION\n",
    "############################################################################\n",
    "\n",
    "def r2_score(true_arr, pred_arr):\n",
    "    ss_res = np.sum((pred_arr - true_arr)**2)\n",
    "    ss_tot = np.sum((true_arr - np.mean(true_arr))**2)\n",
    "    if ss_tot < 1e-9:\n",
    "        return 1.0\n",
    "    return 1.0 - ss_res/ss_tot\n",
    "\n",
    "def dr_ate(alpha_hat, beta_hat, w_arr, y_arr):\n",
    "    mu0 = alpha_hat\n",
    "    mu1 = alpha_hat + beta_hat\n",
    "    e=0.5\n",
    "    IF = (mu1 + w_arr*(y_arr - mu1)/e) - (mu0 + (1-w_arr)*(y_arr - mu0)/(1-e))\n",
    "    ate = IF.mean()\n",
    "    se  = IF.std(ddof=1)/np.sqrt(len(IF))\n",
    "    return ate, se\n",
    "\n",
    "def evaluate_neural(model, dataset, model_type=\"dnn\", max_len=30, action_dim=4, batch_size=256):\n",
    "    \"\"\"\n",
    "    Returns (ate_est, ate_se, r2_y, r2_alpha, r2_beta) for given dataset\n",
    "    \"\"\"\n",
    "    alphaT_list, betaT_list = [], []\n",
    "    alphaP_list, betaP_list = [], []\n",
    "    yT_list, yP_list        = [], []\n",
    "    w_list                  = []\n",
    "\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "        if model_type==\"dnn\":\n",
    "            X_t, W_t, Y_t, A_t, B_t = collate_fn_dnn(batch, max_len, action_dim)\n",
    "            with torch.no_grad():\n",
    "                ab = model(X_t)\n",
    "        elif model_type==\"lstm\":\n",
    "            x_pad, slen, W_t, Y_t, A_t, B_t = collate_fn_lstm(batch, max_len)\n",
    "            with torch.no_grad():\n",
    "                ab = model(x_pad, slen)\n",
    "        else:  # \"transformer\"\n",
    "            x_pad, slen, W_t, Y_t, A_t, B_t = collate_fn_transformer(batch, max_len)\n",
    "            with torch.no_grad():\n",
    "                ab = model(x_pad, slen)\n",
    "        \n",
    "        alpha_hat = ab[:,0].cpu().numpy()\n",
    "        beta_hat  = ab[:,1].cpu().numpy()\n",
    "        \n",
    "        alphaT_list.append(A_t.numpy())\n",
    "        betaT_list.append(B_t.numpy())\n",
    "        \n",
    "        w_arr = W_t.squeeze().numpy()\n",
    "        y_true= Y_t.squeeze().numpy()\n",
    "        y_pred= alpha_hat + beta_hat*w_arr\n",
    "        \n",
    "        alphaP_list.append(alpha_hat)\n",
    "        betaP_list.append(beta_hat)\n",
    "        w_list.append(w_arr)\n",
    "        yT_list.append(y_true)\n",
    "        yP_list.append(y_pred)\n",
    "    \n",
    "    alpha_true = np.concatenate(alphaT_list)\n",
    "    beta_true  = np.concatenate(betaT_list)\n",
    "    alpha_pred = np.concatenate(alphaP_list)\n",
    "    beta_pred  = np.concatenate(betaP_list)\n",
    "    w_arr      = np.concatenate(w_list)\n",
    "    y_true     = np.concatenate(yT_list)\n",
    "    y_pred     = np.concatenate(yP_list)\n",
    "    \n",
    "    ate_est, ate_se = dr_ate(alpha_pred, beta_pred, w_arr, y_true)\n",
    "    r2_y     = r2_score(y_true, y_pred)\n",
    "    r2_alpha = r2_score(alpha_true, alpha_pred)\n",
    "    r2_beta  = r2_score(beta_true,  beta_pred)\n",
    "    return ate_est, ate_se, r2_y, r2_alpha, r2_beta\n",
    "\n",
    "def evaluate_cforest(cf_est, dataset, max_len=30, action_dim=4, batch_size=256):\n",
    "    X_list, beta_true_list = [], []\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "        for (path,w,y,a,b) in batch:\n",
    "            oh = path_to_onehot(path, max_len, action_dim)\n",
    "            X_list.append(oh)\n",
    "            beta_true_list.append(b)\n",
    "    \n",
    "    X_arr = np.array(X_list, dtype=np.float32)\n",
    "    beta_true = np.array(beta_true_list, dtype=np.float32)\n",
    "    \n",
    "    ate_est = cf_est.ate(X=X_arr)\n",
    "    ci_low, ci_high = cf_est.ate_interval(X=X_arr)\n",
    "    ate_se = 0.5*(ci_high - ci_low)\n",
    "    \n",
    "    beta_hat = cf_est.effect(X_arr)\n",
    "    r2_beta  = r2_score(beta_true, beta_hat)\n",
    "    \n",
    "    return ate_est, ate_se, np.nan, np.nan, r2_beta\n",
    "\n",
    "############################################################################\n",
    "# 4) MAIN\n",
    "############################################################################\n",
    "\n",
    "def main():\n",
    "    # Unpack\n",
    "    max_len      = GEN_H[\"max_len\"]\n",
    "    action_dim   = GEN_H[\"action_dim\"]\n",
    "    pop_size     = GEN_H[\"pop_size\"]\n",
    "    sample_train = GEN_H[\"sample_train\"]\n",
    "    sample_test  = GEN_H[\"sample_test\"]\n",
    "    batch_size   = GEN_H[\"batch_size\"]\n",
    "    lr           = GEN_H[\"lr\"]\n",
    "    \n",
    "    dnn_hidden_dim = NN_H[\"dnn_hidden_dim\"]\n",
    "    dnn_num_layers = NN_H[\"dnn_num_layers\"]\n",
    "    dnn_epochs     = NN_H[\"dnn_epochs\"]\n",
    "    \n",
    "    lstm_embed_dim  = NN_H[\"lstm_embed_dim\"]\n",
    "    lstm_hidden_dim = NN_H[\"lstm_hidden_dim\"]\n",
    "    lstm_epochs     = NN_H[\"lstm_epochs\"]\n",
    "    \n",
    "    tf_d_model    = NN_H[\"tf_d_model\"]\n",
    "    tf_nhead      = NN_H[\"tf_nhead\"]\n",
    "    tf_num_layers = NN_H[\"tf_num_layers\"]\n",
    "    tf_epochs     = NN_H[\"tf_epochs\"]\n",
    "    \n",
    "    cf_n_estimators= CF_H[\"n_estimators\"]\n",
    "    cf_max_depth   = CF_H[\"max_depth\"]\n",
    "    cf_num_leaves  = CF_H[\"num_leaves\"]\n",
    "    cf_criterion   = CF_H[\"criterion\"]\n",
    "    \n",
    "    # 1) Build population => minmax => sample train/test\n",
    "    pop_raw = build_population(pop_size, max_len, action_dim)\n",
    "    pop_norm, _ = minmax_normalize_population(pop_raw)\n",
    "    true_ate = compute_true_ate(pop_norm)\n",
    "    print(f\"Population => alpha,beta in [0,1], True ATE= {true_ate:.4f}\")\n",
    "    \n",
    "    full_data = sample_dataset(pop_norm, (sample_train+sample_test), max_len, action_dim)\n",
    "    np.random.shuffle(full_data)\n",
    "    train_data = full_data[:sample_train]\n",
    "    test_data  = full_data[sample_train:]\n",
    "    \n",
    "    # 2) Instantiate DNN, LSTM, Transformer\n",
    "    dnn_model = DNNAlphaBeta(\n",
    "        input_dim=max_len*action_dim,\n",
    "        hidden_dim=dnn_hidden_dim,\n",
    "        num_layers=dnn_num_layers\n",
    "    )\n",
    "    \n",
    "    class LSTMAlphaBeta(nn.Module):\n",
    "        def __init__(self, action_dim=4, embed_dim=4, hidden_dim=4):\n",
    "            super().__init__()\n",
    "            self.embed = nn.Embedding(action_dim, embed_dim)\n",
    "            self.lstm  = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "            self.head  = nn.Linear(hidden_dim, 2)\n",
    "        def forward(self, x_padded, seq_len):\n",
    "            emb = self.embed(x_padded)\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(\n",
    "                emb, seq_len, batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            _, (h_n,_) = self.lstm(packed)\n",
    "            return self.head(h_n[-1])\n",
    "    lstm_model = LSTMAlphaBeta(\n",
    "        action_dim=action_dim,\n",
    "        embed_dim=lstm_embed_dim,\n",
    "        hidden_dim=lstm_hidden_dim\n",
    "    )\n",
    "    \n",
    "    class TransformerAlphaBeta(nn.Module):\n",
    "        def __init__(self, action_dim=4, d_model=8, nhead=1, num_layers=1, max_len=30):\n",
    "            super().__init__()\n",
    "            self.embed = nn.Embedding(action_dim, d_model)\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, nhead=nhead, dim_feedforward=16, batch_first=True\n",
    "            )\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "            self.head = nn.Linear(d_model, 2)\n",
    "            self.max_len = max_len\n",
    "        def forward(self, x_padded, seq_len):\n",
    "            B, L = x_padded.shape\n",
    "            emb = self.embed(x_padded) + self.pos_embed[:, :L, :]\n",
    "            mask = torch.zeros((B, L), dtype=torch.bool, device=emb.device)\n",
    "            for i in range(B):\n",
    "                if seq_len[i]<L:\n",
    "                    mask[i, seq_len[i]:] = True\n",
    "            out_seq = self.transformer(emb, src_key_padding_mask=mask)\n",
    "            out_vec = []\n",
    "            for i in range(B):\n",
    "                plen= seq_len[i].item()\n",
    "                if plen>0:\n",
    "                    out_vec.append(out_seq[i,:plen,:].mean(dim=0))\n",
    "                else:\n",
    "                    out_vec.append(torch.zeros(d_model, device=out_seq.device))\n",
    "            out_vec = torch.stack(out_vec, dim=0)\n",
    "            return self.head(out_vec)\n",
    "    \n",
    "    transformer_model = TransformerAlphaBeta(\n",
    "        action_dim=action_dim,\n",
    "        d_model=tf_d_model,\n",
    "        nhead=tf_nhead,\n",
    "        num_layers=tf_num_layers,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    # 3) Train loops\n",
    "    mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def train_dnn():\n",
    "        opt = optim.Adam(dnn_model.parameters(), lr=lr)\n",
    "        for _ in tqdm(range(dnn_epochs), desc=\"DNN\"):\n",
    "            np.random.shuffle(train_data)\n",
    "            for i in range(0, len(train_data), batch_size):\n",
    "                batch = train_data[i:i+batch_size]\n",
    "                X_t, W_t, Y_t, _, _ = collate_fn_dnn(batch, max_len, action_dim)\n",
    "                ab_pred = dnn_model(X_t)\n",
    "                alpha_hat = ab_pred[:,0:1]\n",
    "                beta_hat  = ab_pred[:,1:2]\n",
    "                Y_hat = alpha_hat + beta_hat*W_t\n",
    "                loss = mse_loss(Y_hat, Y_t)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "    \n",
    "    def train_lstm():\n",
    "        opt = optim.Adam(lstm_model.parameters(), lr=lr)\n",
    "        for _ in tqdm(range(lstm_epochs), desc=\"LSTM\"):\n",
    "            np.random.shuffle(train_data)\n",
    "            for i in range(0, len(train_data), batch_size):\n",
    "                batch = train_data[i:i+batch_size]\n",
    "                x_pad, slen, W_t, Y_t, _, _ = collate_fn_lstm(batch, max_len)\n",
    "                ab_pred = lstm_model(x_pad, slen)\n",
    "                alpha_hat = ab_pred[:,0:1]\n",
    "                beta_hat  = ab_pred[:,1:2]\n",
    "                Y_hat = alpha_hat + beta_hat*W_t\n",
    "                loss = mse_loss(Y_hat, Y_t)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "    \n",
    "    def train_transformer():\n",
    "        opt = optim.Adam(transformer_model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        for _ in tqdm(range(tf_epochs), desc=\"Transformer\"):\n",
    "            np.random.shuffle(train_data)\n",
    "            for i in range(0, len(train_data), batch_size):\n",
    "                batch = train_data[i:i+batch_size]\n",
    "                x_pad, slen, W_t, Y_t, _, _ = collate_fn_transformer(batch, max_len)\n",
    "                ab_pred = transformer_model(x_pad, slen)\n",
    "                alpha_hat = ab_pred[:,0:1]\n",
    "                beta_hat  = ab_pred[:,1:2]\n",
    "                Y_hat = alpha_hat + beta_hat*W_t\n",
    "                loss = mse_loss(Y_hat, Y_t)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "    \n",
    "    # Train them\n",
    "    train_dnn()\n",
    "    train_lstm()\n",
    "    train_transformer()\n",
    "    \n",
    "    # 4) Fit causal forest\n",
    "    def parse_data_cforest(data):\n",
    "        X_list, T_list, Y_list = [], [], []\n",
    "        for (path,w,y,a,b) in data:\n",
    "            oh = path_to_onehot(path, max_len, action_dim)\n",
    "            X_list.append(oh)\n",
    "            T_list.append(w)\n",
    "            Y_list.append(y)\n",
    "        return np.array(X_list, dtype=np.float32), np.array(T_list, dtype=np.float32), np.array(Y_list, dtype=np.float32)\n",
    "    \n",
    "    print(\"\\nFitting CausalForestDML with more leaves...\")\n",
    "    X_train_cf, T_train_cf, Y_train_cf = parse_data_cforest(train_data)\n",
    "    cf_est = CausalForestDML(\n",
    "        model_y=LGBMRegressor(num_leaves=cf_num_leaves, verbose=-1),\n",
    "        model_t=LGBMRegressor(num_leaves=cf_num_leaves, verbose=-1),\n",
    "        discrete_treatment=True,\n",
    "        random_state=123,\n",
    "        n_estimators=cf_n_estimators,\n",
    "        criterion=cf_criterion,\n",
    "        max_depth=cf_max_depth\n",
    "    )\n",
    "    cf_est.fit(Y_train_cf, T_train_cf, X=X_train_cf, W=None)\n",
    "    \n",
    "    # 5) Evaluate both train & test\n",
    "    #    For neural models => (ATE, SE, R2(y), R2(a), R2(b))\n",
    "    #    For CF => (ATE, SE, N/A, N/A, R2(b))\n",
    "    \n",
    "    # Neural nets\n",
    "    dnn_train_res = evaluate_neural(dnn_model, train_data, \"dnn\", max_len, action_dim, batch_size)\n",
    "    dnn_test_res  = evaluate_neural(dnn_model, test_data,  \"dnn\", max_len, action_dim, batch_size)\n",
    "    \n",
    "    lstm_train_res= evaluate_neural(lstm_model, train_data, \"lstm\", max_len, action_dim, batch_size)\n",
    "    lstm_test_res = evaluate_neural(lstm_model, test_data,  \"lstm\", max_len, action_dim, batch_size)\n",
    "    \n",
    "    tf_train_res  = evaluate_neural(transformer_model, train_data,\"transformer\", max_len, action_dim, batch_size)\n",
    "    tf_test_res   = evaluate_neural(transformer_model, test_data, \"transformer\", max_len, action_dim, batch_size)\n",
    "    \n",
    "    # CForest\n",
    "    cf_train_res  = evaluate_cforest(cf_est, train_data, max_len, action_dim, batch_size)\n",
    "    cf_test_res   = evaluate_cforest(cf_est, test_data,  max_len, action_dim, batch_size)\n",
    "    \n",
    "    # 6) Print final table\n",
    "    # We'll do: \n",
    "    #    Model    |  ATE_est(train)  |  R^2(y,train)  |  R^2(a,train)  |  R^2(b,train)  ||  ATE_est(test)  |  R^2(y,test)  |  R^2(a,test)  |  R^2(b,test)\n",
    "    #\n",
    "    # For CF => alpha,y => N/A\n",
    "    def format_r2(x):\n",
    "        if np.isnan(x):\n",
    "            return \"  N/A\"\n",
    "        else:\n",
    "            return f\"{x:7.3f}\"\n",
    "    \n",
    "    # We unify in one table\n",
    "    names = [\"DNN\",\"LSTM\",\"Transformer\",\"CForest\"]\n",
    "    train_res = [dnn_train_res, lstm_train_res, tf_train_res, cf_train_res]\n",
    "    test_res  = [dnn_test_res,  lstm_test_res,  tf_test_res,  cf_test_res]\n",
    "    # each res => (ate, se, r2y, r2a, r2b)\n",
    "\n",
    "    print(\"\\n=== TRAIN vs. TEST RESULTS ===\")\n",
    "    print(\"Model        |    ATE(train)  R^2(y)   R^2(a)   R^2(b)   ||   ATE(test)   R^2(y)   R^2(a)   R^2(b)\")\n",
    "    \n",
    "    for nm, (train_eval, test_eval) in zip(names, zip(train_res, test_res)):\n",
    "        at_tr, se_tr, r2y_tr, r2a_tr, r2b_tr = train_eval\n",
    "        at_te, se_te, r2y_te, r2a_te, r2b_te = test_eval\n",
    "        \n",
    "        # alpha,y => N/A if nan\n",
    "        r2ytr_str = format_r2(r2y_tr)\n",
    "        r2atr_str = format_r2(r2a_tr)\n",
    "        r2btr_str = format_r2(r2b_tr)\n",
    "        \n",
    "        r2yte_str = format_r2(r2y_te)\n",
    "        r2ate_str = format_r2(r2a_te)\n",
    "        r2bte_str = format_r2(r2b_te)\n",
    "        \n",
    "        print(f\"{nm:<12} | {at_tr:10.4f}  {r2ytr_str:>7}  {r2atr_str:>7}  {r2btr_str:>7}  || {at_te:10.4f}  {r2yte_str:>7}  {r2ate_str:>7}  {r2bte_str:>7}\")\n",
    "\n",
    "    print(f\"\\nTrue ATE= {true_ate:.4f}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
