{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq To Reverse List of Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 7.199328899383545\n",
      "Epoch 100, Loss: 0.0003412325750105083\n",
      "Epoch 200, Loss: 7.838086276024114e-06\n",
      "Epoch 300, Loss: 7.689288850087905e-07\n",
      "Epoch 400, Loss: 4.234293982108284e-08\n",
      "Epoch 500, Loss: 1.2916174796373525e-09\n",
      "Epoch 600, Loss: 2.155786660296144e-11\n",
      "Epoch 700, Loss: 3.197442310920451e-13\n",
      "Epoch 800, Loss: 3.552713678800501e-14\n",
      "Epoch 900, Loss: 6.039613253960852e-14\n",
      "Predicted: [4.0000005 3.        1.9999999 1.0000001]\n",
      "Actual: [4. 3. 2. 1.]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Step 1: Define the Seq2Seq Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.GRU(output_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Encoder\n",
    "        _, hidden = self.encoder(src)\n",
    "        \n",
    "        # Decoder\n",
    "        outputs, _ = self.decoder(tgt, hidden)\n",
    "        predictions = self.fc(outputs)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Step 2: Prepare Toy Data\n",
    "# Example: Reverse a sequence of numbers\n",
    "src_data = torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.float32).unsqueeze(-1)  # Input sequence\n",
    "tgt_data = torch.tensor([[5, 4, 3, 2, 1]], dtype=torch.float32).unsqueeze(-1)  # Target sequence (reversed)\n",
    "\n",
    "# Step 3: Initialize Model, Loss, and Optimizer\n",
    "model = Seq2Seq(input_size=1, hidden_size=10, output_size=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Step 4: Train the Model\n",
    "for epoch in range(1000):\n",
    "    model.zero_grad()\n",
    "    output = model(src_data, tgt_data[:, :-1, :])  # Teacher forcing: use tgt as input to decoder\n",
    "    loss = criterion(output, tgt_data[:, 1:, :])   # Compare predictions to shifted tgt\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "# Step 5: Test the Model\n",
    "with torch.no_grad():\n",
    "    test_output = model(src_data, tgt_data[:, :-1, :])\n",
    "    print(\"Predicted:\", test_output.squeeze().numpy())\n",
    "    print(\"Actual:\", tgt_data[:, 1:, :].squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total short pairs: 5000\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = '/Users/pranjal/code/'\n",
    "\n",
    "##############################################################################\n",
    "# 1) LOAD & FILTER TATOEBA DATA (English-French)\n",
    "##############################################################################\n",
    "lang1, lang2 = \"eng\", \"fra\"\n",
    "id2sent = {}\n",
    "\n",
    "with open(path + \"sentences.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for row in reader:\n",
    "        if len(row) == 3:\n",
    "            sid, slang, stext = row\n",
    "            if slang in [lang1, lang2]:\n",
    "                id2sent[sid] = (slang, stext)\n",
    "\n",
    "pairs = []\n",
    "with open(path + \"links.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for row in reader:\n",
    "        if len(row) < 2:\n",
    "            continue\n",
    "        id1, id2 = row\n",
    "        if id1 in id2sent and id2 in id2sent:\n",
    "            la, ta = id2sent[id1]\n",
    "            lb, tb = id2sent[id2]\n",
    "            if la == lang1 and lb == lang2:\n",
    "                pairs.append((ta, tb))\n",
    "            elif la == lang2 and lb == lang1:\n",
    "                pairs.append((tb, ta))\n",
    "\n",
    "# Filter to short sentences only (e.g., <= 50 chars)\n",
    "pairs = [(src, tgt) for (src, tgt) in pairs if len(src) <= 50 and len(tgt) <= 50]\n",
    "\n",
    "# Subsample for quick demo\n",
    "pairs = pairs[:5000]  \n",
    "print(f\"Total short pairs: {len(pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Let's try something.\", 'Essayons quelque chose\\u202f!')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] src_vocab size = 2828 | tgt_vocab size = 3547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/157 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 172\u001b[0m\n\u001b[1;32m    169\u001b[0m tgt_out \u001b[38;5;241m=\u001b[39m tgt_batch[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_in\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (seq_len, batch, vocab_size)\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m    175\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tgt_vocab)), tgt_out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[31], line 93\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, src, tgt_in)\u001b[0m\n\u001b[1;32m     90\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(tgt_emb\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Attention\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     energy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m))  \u001b[38;5;66;03m# (seq_len, batch, hidden_size)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     attention \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv(energy), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (seq_len, batch, 1)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     context \u001b[38;5;241m=\u001b[39m (attention \u001b[38;5;241m*\u001b[39m enc_output)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (batch, 2*hidden_size)\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "##############################################################################\n",
    "# 2) BUILD VOCAB (Word-level)\n",
    "##############################################################################\n",
    "def preprocess(text):\n",
    "    \"\"\"Separate punctuation and lowercase.\"\"\"\n",
    "    text = re.sub(r\"([.!?,])\", r\" \\1\", text.lower())  # Separate punctuation\n",
    "    return text.strip()\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"Create word-level vocabulary with proper preprocessing.\"\"\"\n",
    "    words = set()\n",
    "    for s in sentences:\n",
    "        for w in preprocess(s).split():\n",
    "            words.add(w)\n",
    "    return {\n",
    "        '<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3,\n",
    "        **{w: i+4 for i, w in enumerate(sorted(words))}\n",
    "    }\n",
    "\n",
    "# Build separate vocabs for source (English) and target (French)\n",
    "src_vocab = build_vocab([p[0] for p in pairs])  # English\n",
    "tgt_vocab = build_vocab([p[1] for p in pairs])  # French\n",
    "\n",
    "print(f\"[DEBUG] src_vocab size = {len(src_vocab)} | tgt_vocab size = {len(tgt_vocab)}\")\n",
    "\n",
    "##############################################################################\n",
    "# 3) DATASET & DATALOADER (Word-level)\n",
    "##############################################################################\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, max_len=15):\n",
    "        self.pairs = pairs\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_text, tgt_text = self.pairs[idx]\n",
    "        \n",
    "        def tokenize(text, vocab):\n",
    "            \"\"\"Tokenize with preprocessing and padding.\"\"\"\n",
    "            words = preprocess(text).split()\n",
    "            tokens = ['<start>'] + words + ['<end>']\n",
    "            tokens = tokens[:self.max_len] + ['<pad>']*(self.max_len - len(tokens))\n",
    "            return [vocab.get(w, 1) for w in tokens]  # Use <unk> for unknown words\n",
    "        \n",
    "        src_indices = tokenize(src_text, src_vocab)\n",
    "        tgt_indices = tokenize(tgt_text, tgt_vocab)\n",
    "        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n",
    "\n",
    "dataset = TranslationDataset(pairs, max_len=15)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "##############################################################################\n",
    "# 4) MODEL - Seq2Seq with Attention\n",
    "##############################################################################\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, emb_dim=128, hidden_size=256):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc_emb = nn.Embedding(src_vocab_size, emb_dim)\n",
    "        self.encoder = nn.GRU(emb_dim, hidden_size, bidirectional=True)\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec_emb = nn.Embedding(tgt_vocab_size, emb_dim)\n",
    "        self.decoder = nn.GRU(emb_dim + 2*hidden_size, hidden_size)  # +2*hidden_size for attention\n",
    "        \n",
    "        # Attention\n",
    "        self.attn = nn.Linear(2*hidden_size + hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, tgt_vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt_in):\n",
    "        # Encode\n",
    "        src_emb = self.enc_emb(src)  # (seq_len, batch, emb_dim)\n",
    "        enc_output, hidden = self.encoder(src_emb)  # enc_output: (seq_len, batch, 2*hidden_size)\n",
    "        \n",
    "        # Decode with attention\n",
    "        tgt_emb = self.dec_emb(tgt_in)  # (seq_len, batch, emb_dim)\n",
    "        outputs = []\n",
    "        for t in range(tgt_emb.size(0)):\n",
    "            # Attention\n",
    "            energy = torch.tanh(self.attn(torch.cat((hidden[-1], enc_output), dim=2)))  # (seq_len, batch, hidden_size)\n",
    "            attention = F.softmax(self.v(energy), dim=0)  # (seq_len, batch, 1)\n",
    "            context = (attention * enc_output).sum(dim=0)  # (batch, 2*hidden_size)\n",
    "            \n",
    "            # Decoder step\n",
    "            dec_input = torch.cat((tgt_emb[t], context), dim=1).unsqueeze(0)  # (1, batch, emb_dim + 2*hidden_size)\n",
    "            out, hidden = self.decoder(dec_input, hidden)\n",
    "            outputs.append(self.fc(out))\n",
    "        \n",
    "        return torch.stack(outputs)  # (seq_len, batch, tgt_vocab_size)\n",
    "\n",
    "model = Seq2Seq(len(src_vocab), len(tgt_vocab), emb_dim=128, hidden_size=256)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "##############################################################################\n",
    "# 5) TRANSLATE FUNCTION (Word-level)\n",
    "##############################################################################\n",
    "def translate(model, text, max_len=15):\n",
    "    \"\"\"Greedy decoding with attention.\"\"\"\n",
    "    model.eval()\n",
    "    rev_tgt = {v: k for k, v in tgt_vocab.items()}\n",
    "\n",
    "    # Tokenize source\n",
    "    words = preprocess(text).split()\n",
    "    tokens = ['<start>'] + words + ['<end>']\n",
    "    tokens = tokens[:max_len] + ['<pad>']*(max_len - len(tokens))\n",
    "    src_ids = [src_vocab.get(w, 1) for w in tokens]\n",
    "    src_tensor = torch.tensor(src_ids).unsqueeze(1)  # (seq_len, 1)\n",
    "    \n",
    "    # Encode\n",
    "    with torch.no_grad():\n",
    "        src_emb = model.enc_emb(src_tensor)  # (seq_len, 1, emb_dim)\n",
    "        enc_output, hidden = model.encoder(src_emb)  # enc_output: (seq_len, 1, 2*hidden_size)\n",
    "        \n",
    "        # Decoder init\n",
    "        dec_input = torch.tensor([[tgt_vocab['<start>']]])  # (1, 1)\n",
    "        output_words = []\n",
    "        \n",
    "        for step in range(max_len):\n",
    "            # Attention\n",
    "            energy = torch.tanh(model.attn(torch.cat((hidden[-1], enc_output), dim=2)))  # (seq_len, 1, hidden_size)\n",
    "            attention = F.softmax(model.v(energy), dim=0)  # (seq_len, 1, 1)\n",
    "            context = (attention * enc_output).sum(dim=0)  # (1, 2*hidden_size)\n",
    "            \n",
    "            # Decoder step\n",
    "            dec_emb = model.dec_emb(dec_input)  # (1, 1, emb_dim)\n",
    "            dec_input = torch.cat((dec_emb.squeeze(0), context), dim=1).unsqueeze(0)  # (1, 1, emb_dim + 2*hidden_size)\n",
    "            out, hidden = model.decoder(dec_input, hidden)\n",
    "            logits = model.fc(out).squeeze(0)  # (1, vocab_size) -> (vocab_size,)\n",
    "            \n",
    "            pred_id = logits.argmax(dim=-1).item()\n",
    "            if pred_id == tgt_vocab['<end>'] and step > 3:  # Minimum 3 words\n",
    "                break\n",
    "            output_words.append(rev_tgt.get(pred_id, '<unk>'))\n",
    "            \n",
    "            # Next input is the predicted token\n",
    "            dec_input = torch.tensor([[pred_id]])\n",
    "    \n",
    "    return ' '.join(output_words)\n",
    "\n",
    "##############################################################################\n",
    "# 6) TRAIN LOOP (with debug prints)\n",
    "##############################################################################\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for i, (src_batch, tgt_batch) in enumerate(tqdm(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Reshape to (seq_len, batch)\n",
    "        src_batch = src_batch.permute(1, 0)\n",
    "        tgt_in = tgt_batch[:, :-1].permute(1, 0)\n",
    "        tgt_out = tgt_batch[:, 1:]\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(src_batch, tgt_in)  # (seq_len, batch, vocab_size)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.reshape(-1, len(tgt_vocab)), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Debug on first batch\n",
    "        if i == 0:\n",
    "            print(f\"[DEBUG] EPOCH {epoch+1}, BATCH=0, loss={loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    scheduler.step(avg_loss)  # Update learning rate\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Quick test\n",
    "    sample_src, sample_tgt = random.choice(pairs)\n",
    "    prediction = translate(model, sample_src)\n",
    "    print(f\"[DEBUG] Sample Source: {sample_src}\")\n",
    "    print(f\"[DEBUG] True Target:   {sample_tgt}\")\n",
    "    print(f\"[DEBUG] Prediction:    {prediction}\")\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
