{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language models\n",
    "- Goal is to predict the next token from the context. \n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "### Negative Log-Likelihood (NLL)  \n",
    "$ \\text{NLL} = -\\sum_{i=1}^N \\log P(x_i) $  \n",
    "- Directly tied to the probability model.  \n",
    "- Standard metric in language modeling.  \n",
    "- Harder to interpret (no direct intuitive scale).  \n",
    "\n",
    "### Perplexity (PP)  \n",
    "$ \\text{PP} = \\exp\\!\\Bigl(-\\frac{1}{N} \\sum_{i=1}^N \\log P(x_i)\\Bigr) $  \n",
    "- Easy to interpret as an “effective branching factor.”  \n",
    "- Most common in language modeling.  \n",
    "- Log scale can hide small differences.  \n",
    "- Does not show which errors the model makes.  \n",
    "\n",
    "### Cross-Entropy (CE)  \n",
    "$ H(P, Q) = -\\sum_{x} P(x)\\,\\log Q(x) $  \n",
    "- Same as NLL but often measured in bits or nats.  \n",
    "- Has a clear information-theoretic interpretation.  \n",
    "- Lacks a simple “accuracy”-like interpretation.  \n",
    "\n",
    "### Accuracy  \n",
    "$ \\text{Accuracy} = \\frac{\\# \\text{correct predictions}}{\\# \\text{total predictions}} $  \n",
    "- Simple and intuitive.  \n",
    "- Not very informative in language modeling; ignores probability distributions over all possible tokens.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Level Count Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Train Text Length: 12000\n",
      "Test Text Length:       3000\n",
      "\n",
      "| Data            |   Avg NLL (bits) |   Perplexity |   Accuracy |\n",
      "|-----------------|------------------|--------------|------------|\n",
      "| N=100 (Train)   |            1.667 |        3.176 |      0.475 |\n",
      "| N=100 (Test)    |            4.085 |       16.967 |      0.145 |\n",
      "| N=1000 (Train)  |            3.062 |        8.352 |      0.325 |\n",
      "| N=1000 (Test)   |            3.648 |       12.533 |      0.256 |\n",
      "| N=5000 (Train)  |            3.432 |       10.795 |      0.285 |\n",
      "| N=5000 (Test)   |            3.571 |       11.888 |      0.265 |\n",
      "| N=12000 (Train) |            3.459 |       10.998 |      0.285 |\n",
      "| N=12000 (Test)  |            3.503 |       11.337 |      0.268 |\n",
      "\n",
      "Example Next-Character Probabilities (using largest N model):\n",
      "Context: ''T'' => Next Char Probabilities (top 5):\n",
      "   1. 'h' -> 0.6869\n",
      "   2. 'r' -> 0.1010\n",
      "   3. 'i' -> 0.0909\n",
      "   4. 'o' -> 0.0707\n",
      "   5. 'e' -> 0.0303\n",
      "\n",
      "Context: '' '' => Next Char Probabilities (top 5):\n",
      "   1. 't' -> 0.1148\n",
      "   2. ' ' -> 0.0718\n",
      "   3. 'h' -> 0.0607\n",
      "   4. 'a' -> 0.0597\n",
      "   5. 's' -> 0.0526\n",
      "\n",
      "Context: ''\\n'' => Next Char Probabilities (top 5):\n",
      "   1. '\\n' -> 0.2153\n",
      "   2. ' ' -> 0.2030\n",
      "   3. 'A' -> 0.0965\n",
      "   4. 'T' -> 0.0965\n",
      "   5. 'W' -> 0.0644\n",
      "\n",
      "Context: ''.'' => Next Char Probabilities (top 5):\n",
      "   1. ' ' -> 0.6884\n",
      "   2. '\\n' -> 0.3116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import math\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Ensure you've downloaded NLTK's Gutenberg corpus once:\n",
    "# nltk.download('gutenberg')\n",
    "\n",
    "# 1) Load a short text: Shakespeare's \"Macbeth\" (approx 100k chars total).\n",
    "raw_text = gutenberg.raw('shakespeare-macbeth.txt')\n",
    "\n",
    "# Truncate to ~15k chars for a small demonstration\n",
    "raw_text = raw_text[:15000]\n",
    "\n",
    "# Train/test split\n",
    "split_idx = int(len(raw_text) * 0.8)\n",
    "full_train_text = raw_text[:split_idx]\n",
    "test_text = raw_text[split_idx:]\n",
    "\n",
    "def build_bigram_model(text):\n",
    "    \"\"\"Count single chars & bigrams, compute probabilities.\"\"\"\n",
    "    char_counts = Counter(text)\n",
    "    bigram_counts = Counter()\n",
    "    for i in range(len(text) - 1):\n",
    "        bigram = text[i : i + 2]\n",
    "        bigram_counts[bigram] += 1\n",
    "    \n",
    "    bigram_prob = {}\n",
    "    for bg, cnt in bigram_counts.items():\n",
    "        curr_char = bg[0]\n",
    "        # Probability = count(bigram) / count(curr_char)\n",
    "        bigram_prob[bg] = cnt / char_counts[curr_char]\n",
    "    \n",
    "    return bigram_prob, char_counts\n",
    "\n",
    "def evaluate(text, bigram_prob, char_counts):\n",
    "    \"\"\"Compute NLL (bits), Perplexity, and Accuracy on a given text.\"\"\"\n",
    "    # Precompute top next-char for accuracy\n",
    "    best_next_char = {}\n",
    "    for bg, prob in bigram_prob.items():\n",
    "        c, nxt = bg[0], bg[1]\n",
    "        if c not in best_next_char or prob > best_next_char[c][1]:\n",
    "            best_next_char[c] = (nxt, prob)\n",
    "    \n",
    "    total_NLL_bits = 0.0\n",
    "    correct = 0\n",
    "    total_bigrams = 0\n",
    "    \n",
    "    for i in range(len(text) - 1):\n",
    "        c = text[i]\n",
    "        nxt = text[i + 1]\n",
    "        bg = c + nxt\n",
    "        \n",
    "        if bg in bigram_prob:\n",
    "            p = bigram_prob[bg]\n",
    "        else:\n",
    "            # Unseen fallback probability\n",
    "            p = 1.0 / max(len(char_counts), 1)\n",
    "        \n",
    "        total_NLL_bits += -math.log2(p)\n",
    "        total_bigrams += 1\n",
    "        \n",
    "        # Accuracy check\n",
    "        if c in best_next_char:\n",
    "            pred_char, _ = best_next_char[c]\n",
    "            if pred_char == nxt:\n",
    "                correct += 1\n",
    "    \n",
    "    if total_bigrams == 0:\n",
    "        return float('inf'), float('inf'), 0.0\n",
    "    \n",
    "    avg_NLL_bits = total_NLL_bits / total_bigrams\n",
    "    perplexity = 2 ** avg_NLL_bits\n",
    "    accuracy = correct / total_bigrams\n",
    "    return avg_NLL_bits, perplexity, accuracy\n",
    "\n",
    "def show_next_char_probs(context_char, bigram_prob, top_n=5):\n",
    "    \"\"\"Display the top-N next-character probabilities for a given single-char context.\"\"\"\n",
    "    # Gather all bigrams starting with `context_char`\n",
    "    candidates = [(bg[1], p) for bg, p in bigram_prob.items() if bg[0] == context_char]\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if not candidates:\n",
    "        print(f\"No learned next chars for context '{repr(context_char)}'\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Context: '{repr(context_char)}' => Next Char Probabilities (top {top_n}):\")\n",
    "    for rank, (ch, prob) in enumerate(candidates[:top_n], 1):\n",
    "        visible_char = repr(ch) if ch in ['\\n', ' '] else f\"'{ch}'\"\n",
    "        print(f\"  {rank:2d}. {visible_char} -> {prob:.4f}\")\n",
    "    print()\n",
    "\n",
    "# 2) Try different N sizes for training\n",
    "train_sizes = [100, 1000, 5000, len(full_train_text)]\n",
    "rows = []\n",
    "models_stored = {}  # to optionally inspect models afterwards\n",
    "\n",
    "for N in train_sizes:\n",
    "    partial_train_text = full_train_text[:N]\n",
    "    \n",
    "    # Build model on partial train\n",
    "    bigram_prob, char_counts = build_bigram_model(partial_train_text)\n",
    "    models_stored[N] = (bigram_prob, char_counts)\n",
    "    \n",
    "    # Evaluate on partial train\n",
    "    train_nll, train_pp, train_acc = evaluate(partial_train_text, bigram_prob, char_counts)\n",
    "    \n",
    "    # Evaluate on test\n",
    "    test_nll, test_pp, test_acc = evaluate(test_text, bigram_prob, char_counts)\n",
    "    \n",
    "    rows.append([\n",
    "        f\"N={N} (Train)\",\n",
    "        f\"{train_nll:.3f}\",\n",
    "        f\"{train_pp:.3f}\",\n",
    "        f\"{train_acc:.3f}\"\n",
    "    ])\n",
    "    rows.append([\n",
    "        f\"N={N} (Test)\",\n",
    "        f\"{test_nll:.3f}\",\n",
    "        f\"{test_pp:.3f}\",\n",
    "        f\"{test_acc:.3f}\"\n",
    "    ])\n",
    "\n",
    "# 3) Print results table\n",
    "print(f\"Full Train Text Length: {len(full_train_text)}\")\n",
    "print(f\"Test Text Length:       {len(test_text)}\\n\")\n",
    "\n",
    "headers = [\"Data\", \"Avg NLL (bits)\", \"Perplexity\", \"Accuracy\"]\n",
    "print(tabulate(rows, headers=headers, tablefmt=\"github\"))\n",
    "\n",
    "# 4) Show example next-char probabilities using the largest trained model\n",
    "bigram_prob_final, _ = models_stored[len(full_train_text)]\n",
    "print(\"\\nExample Next-Character Probabilities (using largest N model):\")\n",
    "for ctx_char in [\"T\", \" \", \"\\n\", \".\"]:\n",
    "    show_next_char_probs(ctx_char, bigram_prob_final, top_n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char Level RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stage 1] Training on 9940 samples (size=10000).\n",
      "Epoch 1/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.1517 - loss: 3.5174 - val_accuracy: 0.2002 - val_loss: 3.1286\n",
      "Epoch 2/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - accuracy: 0.2227 - loss: 2.9777 - val_accuracy: 0.2535 - val_loss: 2.7969\n",
      "Epoch 3/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - accuracy: 0.2660 - loss: 2.6602 - val_accuracy: 0.2636 - val_loss: 2.6455\n",
      "Epoch 4/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - accuracy: 0.3024 - loss: 2.4930 - val_accuracy: 0.2847 - val_loss: 2.5592\n",
      "Epoch 5/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - accuracy: 0.3202 - loss: 2.3642 - val_accuracy: 0.2988 - val_loss: 2.4705\n",
      "Epoch 6/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - accuracy: 0.3378 - loss: 2.3050 - val_accuracy: 0.3179 - val_loss: 2.4365\n",
      "Epoch 7/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - accuracy: 0.3699 - loss: 2.2384 - val_accuracy: 0.3350 - val_loss: 2.3889\n",
      "Epoch 8/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - accuracy: 0.3736 - loss: 2.1916 - val_accuracy: 0.3501 - val_loss: 2.3528\n",
      "Epoch 9/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - accuracy: 0.3910 - loss: 2.1393 - val_accuracy: 0.3531 - val_loss: 2.3451\n",
      "Epoch 10/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - accuracy: 0.4006 - loss: 2.0902 - val_accuracy: 0.3682 - val_loss: 2.3204\n",
      "[Stage 1] Val Loss (nats): 2.320\n",
      "[Stage 1] Val Loss (bits): 3.348\n",
      "[Stage 1] Val Perplexity:  10.180\n",
      "[Stage 1] Val Accuracy:    0.368\n",
      "\n",
      "[Stage 1] Sample text (prompt: 'MACBETH'):\n",
      "MACBETHocu. the The Lath alll the why sraud polr the Kind\n",
      "Shat,\n",
      "Ird Thae pamerd, rii the shous rhyqryam,\n",
      "Whiu is the faller pfonpdos,\n",
      "Whof a sout\n",
      "Derot the thacd wigke -ning. Sot hire rrofne,\n",
      "Vreat the dum t\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Stage 2] Training on 19940 samples (size=20000).\n",
      "Epoch 1/10\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - accuracy: 0.3885 - loss: 2.1477 - val_accuracy: 0.3696 - val_loss: 2.2534\n",
      "Epoch 2/10\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - accuracy: 0.4071 - loss: 2.0865 - val_accuracy: 0.3701 - val_loss: 2.2126\n",
      "Epoch 3/10\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - accuracy: 0.4221 - loss: 2.0203 - val_accuracy: 0.3862 - val_loss: 2.1782\n",
      "Epoch 4/10\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - accuracy: 0.4305 - loss: 1.9805 - val_accuracy: 0.3872 - val_loss: 2.1372\n",
      "Epoch 5/10\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - accuracy: 0.4460 - loss: 1.9248 - val_accuracy: 0.3917 - val_loss: 2.1155\n",
      "Epoch 6/10\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 34ms/step - accuracy: 0.4462 - loss: 1.8915 - val_accuracy: 0.3852 - val_loss: 2.0974\n",
      "Epoch 7/10\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - accuracy: 0.4530 - loss: 1.8680 - val_accuracy: 0.3997 - val_loss: 2.0798\n",
      "Epoch 8/10\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - accuracy: 0.4603 - loss: 1.8505 - val_accuracy: 0.4067 - val_loss: 2.0552\n",
      "Epoch 9/10\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - accuracy: 0.4735 - loss: 1.7931 - val_accuracy: 0.4142 - val_loss: 2.0485\n",
      "Epoch 10/10\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - accuracy: 0.4843 - loss: 1.7619 - val_accuracy: 0.4082 - val_loss: 2.0233\n",
      "[Stage 2] Val Loss (nats): 2.023\n",
      "[Stage 2] Val Loss (bits): 2.919\n",
      "[Stage 2] Val Perplexity:  7.563\n",
      "[Stage 2] Val Accuracy:    0.408\n",
      "\n",
      "[Stage 2] Sample text (prompt: 'MACBETH'):\n",
      "MACBETHer. Dhos thing heeminge: To mids,\n",
      "Angu. Entere, oy my vssourd, prinme bes waylie at of me speare ongo desteres,\n",
      "sy not to pistou:\n",
      "The re I peplestemeas.\n",
      "\n",
      "   Macb. Y hee the mefe,\n",
      "As of mawe stand, Int\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Stage 3] Training on 29940 samples (size=30000).\n",
      "Epoch 1/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.4753 - loss: 1.8025 - val_accuracy: 0.4272 - val_loss: 2.0101\n",
      "Epoch 2/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 35ms/step - accuracy: 0.4812 - loss: 1.7619 - val_accuracy: 0.4419 - val_loss: 1.9898\n",
      "Epoch 3/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 35ms/step - accuracy: 0.4888 - loss: 1.7350 - val_accuracy: 0.4349 - val_loss: 1.9915\n",
      "Epoch 4/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 34ms/step - accuracy: 0.5048 - loss: 1.6916 - val_accuracy: 0.4482 - val_loss: 1.9507\n",
      "Epoch 5/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 35ms/step - accuracy: 0.5045 - loss: 1.6738 - val_accuracy: 0.4466 - val_loss: 1.9484\n",
      "Epoch 6/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 35ms/step - accuracy: 0.5071 - loss: 1.6578 - val_accuracy: 0.4449 - val_loss: 1.9374\n",
      "Epoch 7/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - accuracy: 0.5214 - loss: 1.6236 - val_accuracy: 0.4576 - val_loss: 1.9152\n",
      "Epoch 8/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - accuracy: 0.5226 - loss: 1.6020 - val_accuracy: 0.4613 - val_loss: 1.9077\n",
      "Epoch 9/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 35ms/step - accuracy: 0.5329 - loss: 1.5717 - val_accuracy: 0.4546 - val_loss: 1.9060\n",
      "Epoch 10/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 35ms/step - accuracy: 0.5425 - loss: 1.5400 - val_accuracy: 0.4506 - val_loss: 1.9086\n",
      "[Stage 3] Val Loss (nats): 1.909\n",
      "[Stage 3] Val Loss (bits): 2.754\n",
      "[Stage 3] Val Perplexity:  6.744\n",
      "[Stage 3] Val Accuracy:    0.451\n",
      "\n",
      "[Stage 3] Sample text (prompt: 'MACBETH'):\n",
      "MACBETHel. Hast, Bnatter, thou pike\n",
      "Vingeg ale manke. Scencnacans. With Selent Haine, fort,\n",
      "Ore pakeon, my vnmorst.\n",
      "\n",
      " Banq. Were ole to they no muncan:\n",
      "Leule it wis\n",
      "\n",
      "   Lestere: Banou: Withinne.\n",
      "Ant Fighes. \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Stage 4] Training on 49940 samples (size=50000).\n",
      "Epoch 1/10\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 33ms/step - accuracy: 0.5132 - loss: 1.6710 - val_accuracy: 0.4846 - val_loss: 1.7932\n",
      "Epoch 2/10\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 32ms/step - accuracy: 0.5248 - loss: 1.6212 - val_accuracy: 0.4886 - val_loss: 1.7678\n",
      "Epoch 3/10\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.5366 - loss: 1.5748 - val_accuracy: 0.4924 - val_loss: 1.7564\n",
      "Epoch 4/10\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 32ms/step - accuracy: 0.5411 - loss: 1.5470 - val_accuracy: 0.4984 - val_loss: 1.7515\n",
      "Epoch 5/10\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 32ms/step - accuracy: 0.5497 - loss: 1.5294 - val_accuracy: 0.4970 - val_loss: 1.7474\n",
      "Epoch 6/10\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 35ms/step - accuracy: 0.5505 - loss: 1.5117 - val_accuracy: 0.5024 - val_loss: 1.7422\n",
      "Epoch 7/10\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 34ms/step - accuracy: 0.5666 - loss: 1.4726 - val_accuracy: 0.5028 - val_loss: 1.7376\n",
      "Epoch 8/10\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 33ms/step - accuracy: 0.5661 - loss: 1.4624 - val_accuracy: 0.4998 - val_loss: 1.7484\n",
      "Epoch 9/10\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.5756 - loss: 1.4375 - val_accuracy: 0.4968 - val_loss: 1.7445\n",
      "Epoch 10/10\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 34ms/step - accuracy: 0.5834 - loss: 1.4027 - val_accuracy: 0.4930 - val_loss: 1.7588\n",
      "[Stage 4] Val Loss (nats): 1.759\n",
      "[Stage 4] Val Loss (bits): 2.537\n",
      "[Stage 4] Val Perplexity:  5.805\n",
      "[Stage 4] Val Accuracy:    0.493\n",
      "\n",
      "[Stage 4] Sample text (prompt: 'MACBETH'):\n",
      "MACBETHa. There\n",
      "A habe you shall things as hustall Sich and Thanknes of hadge, and Cawdor,\n",
      "Thou inst op Robors like a yoe, whose thour?\n",
      "  Macd. So me bad Roch liue dere-coan man.\n",
      "\n",
      "  Connot Leace.\n",
      "What ewall \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# -----------------------\n",
    "# 1) Load Macbeth\n",
    "# -----------------------\n",
    "nltk.download('gutenberg', quiet=True)\n",
    "\n",
    "raw_text = gutenberg.raw('shakespeare-macbeth.txt')\n",
    "\n",
    "# Let's truncate to ~50k chars for demonstration (adjust as you wish).\n",
    "raw_text = raw_text[:50000]\n",
    "\n",
    "# We'll define a function to train on partial sizes:\n",
    "partial_sizes = [10000, 20000, 30000, len(raw_text)]  # incremental\n",
    "\n",
    "# -----------------------\n",
    "# 2) Hyperparameters\n",
    "# -----------------------\n",
    "SEQ_LEN = 60        # length of input sequence\n",
    "EMBED_DIM = 64      # embedding size\n",
    "RNN_UNITS = 128     # size of the RNN hidden state\n",
    "EPOCHS_PER_STAGE = 10  # how many epochs to train at each stage\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# -----------------------\n",
    "# 3) Build a RNN Model\n",
    "# -----------------------\n",
    "def build_char_model(vocab_size):\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(input_dim=vocab_size, \n",
    "                         output_dim=EMBED_DIM, \n",
    "                         input_length=SEQ_LEN),\n",
    "        layers.LSTM(RNN_UNITS, return_sequences=False),\n",
    "        layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -----------------------\n",
    "# 4) Preprocessing\n",
    "# -----------------------\n",
    "def create_vocab(text):\n",
    "    \"\"\"Returns char2idx, idx2char for the given text.\"\"\"\n",
    "    chars = sorted(set(text))\n",
    "    char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx2char = {i: ch for ch, i in char2idx.items()}\n",
    "    return char2idx, idx2char\n",
    "\n",
    "def build_dataset(text, seq_len, char2idx):\n",
    "    \"\"\"\n",
    "    Convert text into (X, y) pairs:\n",
    "      - X[i] is seq_len chars, \n",
    "      - y[i] is the next char \n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(text) - seq_len):\n",
    "        seq = text[i : i + seq_len]\n",
    "        nxt = text[i + seq_len]\n",
    "        X.append([char2idx[ch] for ch in seq])\n",
    "        y.append(char2idx[nxt])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def sample_text(model, start_text, char2idx, idx2char, length=200):\n",
    "    \"\"\"\n",
    "    Generate text from the model given a starting prompt.\n",
    "    Uses the last SEQ_LEN chars as context for each step.\n",
    "    \"\"\"\n",
    "    text_out = start_text\n",
    "    for _ in range(length):\n",
    "        # Take the last SEQ_LEN chars as input (pad if shorter)\n",
    "        context = text_out[-SEQ_LEN:]\n",
    "        if len(context) < SEQ_LEN:\n",
    "            context = ' ' * (SEQ_LEN - len(context)) + context\n",
    "        \n",
    "        # Encode to IDs\n",
    "        x_in = [char2idx.get(ch, 0) for ch in context]\n",
    "        x_in = np.array([x_in])  # shape (1, SEQ_LEN)\n",
    "\n",
    "        # Predict distribution\n",
    "        preds = model.predict(x_in, verbose=0)[0]  # shape (vocab_size,)\n",
    "\n",
    "        # Sample from the distribution\n",
    "        next_idx = np.random.choice(np.arange(len(preds)), p=preds)\n",
    "        next_char = idx2char[next_idx]\n",
    "        text_out += next_char\n",
    "    return text_out\n",
    "\n",
    "# -----------------------\n",
    "# Main Logic\n",
    "# -----------------------\n",
    "# We'll build one model and re-train it at each stage.\n",
    "# Alternatively, you could build a new model from scratch at each stage, \n",
    "# but continuing training might show how performance changes as we add more data.\n",
    "char2idx_full, idx2char_full = create_vocab(raw_text)\n",
    "vocab_size = len(char2idx_full)\n",
    "model = build_char_model(vocab_size)\n",
    "\n",
    "previous_end = 0\n",
    "\n",
    "for stage_idx, size in enumerate(partial_sizes, start=1):\n",
    "    # 1) Extract partial text [0 : size]\n",
    "    partial_text = raw_text[:size]\n",
    "\n",
    "    # 2) Build dataset\n",
    "    #    We must use the full vocabulary from the entire text, \n",
    "    #    or you can build a new vocab each time. \n",
    "    X_data, y_data = build_dataset(partial_text, SEQ_LEN, char2idx_full)\n",
    "\n",
    "    # 3) Train/Val split (e.g. 90/10 for demonstration)\n",
    "    split_i = int(0.9 * len(X_data))\n",
    "    X_train, X_val = X_data[:split_i], X_data[split_i:]\n",
    "    y_train, y_val = y_data[:split_i], y_data[split_i:]\n",
    "\n",
    "    if len(X_train) == 0:\n",
    "        print(f\"\\n[Stage {stage_idx}] Not enough data to train (size={size}). Skipping.\\n\")\n",
    "        continue\n",
    "\n",
    "    # 4) Train for some epochs\n",
    "    print(f\"\\n[Stage {stage_idx}] Training on {len(X_data)} samples (size={size}).\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS_PER_STAGE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 5) Evaluate perplexity on val set\n",
    "    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "    val_loss_bits = val_loss / math.log(2)\n",
    "    val_pp = 2 ** val_loss_bits\n",
    "\n",
    "    print(f\"[Stage {stage_idx}] Val Loss (nats): {val_loss:.3f}\")\n",
    "    print(f\"[Stage {stage_idx}] Val Loss (bits): {val_loss_bits:.3f}\")\n",
    "    print(f\"[Stage {stage_idx}] Val Perplexity:  {val_pp:.3f}\")\n",
    "    print(f\"[Stage {stage_idx}] Val Accuracy:    {val_acc:.3f}\")\n",
    "\n",
    "    # 6) Generate text from a prompt\n",
    "    prompt = \"MACBETH\"\n",
    "    gen = sample_text(model, prompt, char2idx_full, idx2char_full, length=200)\n",
    "    print(f\"\\n[Stage {stage_idx}] Sample text (prompt: '{prompt}'):\\n{gen}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char Level LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stage 1] Training on 9940 samples (size=10000).\n",
      "Epoch 1/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.1552 - loss: 3.5300 - val_accuracy: 0.1700 - val_loss: 3.1257\n",
      "Epoch 2/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.2130 - loss: 3.0135 - val_accuracy: 0.2596 - val_loss: 2.7582\n",
      "Epoch 3/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.2861 - loss: 2.6178 - val_accuracy: 0.2757 - val_loss: 2.5914\n",
      "Epoch 4/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - accuracy: 0.3129 - loss: 2.4314 - val_accuracy: 0.2877 - val_loss: 2.5219\n",
      "Epoch 5/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.3279 - loss: 2.3343 - val_accuracy: 0.2938 - val_loss: 2.4778\n",
      "Epoch 6/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.3448 - loss: 2.2721 - val_accuracy: 0.3199 - val_loss: 2.4245\n",
      "Epoch 7/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - accuracy: 0.3688 - loss: 2.2169 - val_accuracy: 0.3390 - val_loss: 2.3950\n",
      "Epoch 8/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - accuracy: 0.3688 - loss: 2.1810 - val_accuracy: 0.3380 - val_loss: 2.3746\n",
      "Epoch 9/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.4018 - loss: 2.1143 - val_accuracy: 0.3571 - val_loss: 2.3465\n",
      "Epoch 10/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.3919 - loss: 2.1067 - val_accuracy: 0.3551 - val_loss: 2.3150\n",
      "Epoch 11/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - accuracy: 0.4119 - loss: 2.0661 - val_accuracy: 0.3622 - val_loss: 2.3049\n",
      "Epoch 12/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.4218 - loss: 2.0198 - val_accuracy: 0.3481 - val_loss: 2.2937\n",
      "Epoch 13/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - accuracy: 0.4327 - loss: 1.9779 - val_accuracy: 0.3652 - val_loss: 2.2789\n",
      "Epoch 14/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - accuracy: 0.4420 - loss: 1.9444 - val_accuracy: 0.3622 - val_loss: 2.2630\n",
      "Epoch 15/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.4552 - loss: 1.8912 - val_accuracy: 0.3732 - val_loss: 2.2456\n",
      "Epoch 16/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.4496 - loss: 1.8877 - val_accuracy: 0.3803 - val_loss: 2.2209\n",
      "Epoch 17/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.4779 - loss: 1.8084 - val_accuracy: 0.3873 - val_loss: 2.2156\n",
      "Epoch 18/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.4717 - loss: 1.8269 - val_accuracy: 0.3853 - val_loss: 2.2161\n",
      "Epoch 19/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.4839 - loss: 1.7683 - val_accuracy: 0.3732 - val_loss: 2.2154\n",
      "Epoch 20/20\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.4826 - loss: 1.7398 - val_accuracy: 0.3813 - val_loss: 2.1963\n",
      "[Stage 1] Val Loss (nats): 2.196\n",
      "[Stage 1] Val Loss (bits): 3.169\n",
      "[Stage 1] Val Perplexity:  8.991\n",
      "[Stage 1] Val Accuracy:    0.381\n",
      "\n",
      "[Stage 1] Sample text (prompt: 'MACBETH'):\n",
      "MACBETHu. Tha 3nor with bpant?\n",
      "   1. Tpent vistaue Ertiunt Womlort, and hane orp his,\n",
      "And part,\n",
      "Cwiding s'ige noteeall Hickes;\n",
      "(nt sha'd frist menqunds,\n",
      "Postlit halt of Nompor? 3 Thank'd,\n",
      "Sqot of pip youe th\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Stage 2] Training on 19940 samples (size=20000).\n",
      "Epoch 1/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - accuracy: 0.4520 - loss: 1.9053 - val_accuracy: 0.3796 - val_loss: 2.1260\n",
      "Epoch 2/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 33ms/step - accuracy: 0.4642 - loss: 1.8413 - val_accuracy: 0.3932 - val_loss: 2.1056\n",
      "Epoch 3/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - accuracy: 0.4792 - loss: 1.7925 - val_accuracy: 0.3997 - val_loss: 2.0903\n",
      "Epoch 4/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.4837 - loss: 1.7591 - val_accuracy: 0.4057 - val_loss: 2.0558\n",
      "Epoch 5/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - accuracy: 0.4919 - loss: 1.7282 - val_accuracy: 0.4067 - val_loss: 2.0492\n",
      "Epoch 6/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - accuracy: 0.5024 - loss: 1.6879 - val_accuracy: 0.4102 - val_loss: 2.0376\n",
      "Epoch 7/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.5138 - loss: 1.6494 - val_accuracy: 0.4112 - val_loss: 2.0320\n",
      "Epoch 8/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.5261 - loss: 1.6087 - val_accuracy: 0.4243 - val_loss: 2.0270\n",
      "Epoch 9/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 34ms/step - accuracy: 0.5333 - loss: 1.5856 - val_accuracy: 0.4213 - val_loss: 2.0149\n",
      "Epoch 10/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - accuracy: 0.5442 - loss: 1.5372 - val_accuracy: 0.4263 - val_loss: 2.0058\n",
      "Epoch 11/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - accuracy: 0.5590 - loss: 1.4894 - val_accuracy: 0.4213 - val_loss: 2.0243\n",
      "Epoch 12/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - accuracy: 0.5664 - loss: 1.4602 - val_accuracy: 0.4283 - val_loss: 2.0032\n",
      "Epoch 13/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - accuracy: 0.5676 - loss: 1.4619 - val_accuracy: 0.4323 - val_loss: 2.0038\n",
      "Epoch 14/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - accuracy: 0.5754 - loss: 1.4282 - val_accuracy: 0.4258 - val_loss: 1.9950\n",
      "Epoch 15/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 34ms/step - accuracy: 0.5925 - loss: 1.3727 - val_accuracy: 0.4403 - val_loss: 1.9933\n",
      "Epoch 16/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - accuracy: 0.6043 - loss: 1.3314 - val_accuracy: 0.4288 - val_loss: 2.0230\n",
      "Epoch 17/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - accuracy: 0.6096 - loss: 1.3196 - val_accuracy: 0.4308 - val_loss: 2.0236\n",
      "Epoch 18/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 34ms/step - accuracy: 0.6210 - loss: 1.2919 - val_accuracy: 0.4263 - val_loss: 2.0443\n",
      "Epoch 19/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - accuracy: 0.6287 - loss: 1.2578 - val_accuracy: 0.4268 - val_loss: 2.0271\n",
      "Epoch 20/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - accuracy: 0.6412 - loss: 1.2214 - val_accuracy: 0.4293 - val_loss: 2.0631\n",
      "[Stage 2] Val Loss (nats): 2.063\n",
      "[Stage 2] Val Loss (bits): 2.976\n",
      "[Stage 2] Val Perplexity:  7.870\n",
      "[Stage 2] Val Accuracy:    0.429\n",
      "\n",
      "[Stage 2] Sample text (prompt: 'MACBETH'):\n",
      "MACBETHmult0' Agnon him,? wh' Shato worrotht the reace,\n",
      "My loe gryaue, I wandey it\n",
      "Tho proses ir'd, tell he dould egadt Ninguen,; Poste, will he doe, the Durberarions Varine: which sweake\n",
      "\n",
      "   2. Where though\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Stage 3] Training on 29940 samples (size=30000).\n",
      "Epoch 1/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 32ms/step - accuracy: 0.5745 - loss: 1.4964 - val_accuracy: 0.4299 - val_loss: 2.0661\n",
      "Epoch 2/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.5894 - loss: 1.4223 - val_accuracy: 0.4315 - val_loss: 2.0561\n",
      "Epoch 3/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.5987 - loss: 1.3810 - val_accuracy: 0.4315 - val_loss: 2.0680\n",
      "Epoch 4/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.5979 - loss: 1.3656 - val_accuracy: 0.4389 - val_loss: 2.0712\n",
      "Epoch 5/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.6080 - loss: 1.3429 - val_accuracy: 0.4262 - val_loss: 2.0797\n",
      "Epoch 6/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.6139 - loss: 1.3056 - val_accuracy: 0.4292 - val_loss: 2.0788\n",
      "Epoch 7/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 35ms/step - accuracy: 0.6262 - loss: 1.2720 - val_accuracy: 0.4265 - val_loss: 2.1008\n",
      "Epoch 8/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 35ms/step - accuracy: 0.6349 - loss: 1.2433 - val_accuracy: 0.4345 - val_loss: 2.1022\n",
      "Epoch 9/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 35ms/step - accuracy: 0.6460 - loss: 1.2097 - val_accuracy: 0.4248 - val_loss: 2.1140\n",
      "Epoch 10/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 35ms/step - accuracy: 0.6412 - loss: 1.2159 - val_accuracy: 0.4265 - val_loss: 2.1071\n",
      "Epoch 11/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.6550 - loss: 1.1751 - val_accuracy: 0.4232 - val_loss: 2.1473\n",
      "Epoch 12/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 35ms/step - accuracy: 0.6649 - loss: 1.1425 - val_accuracy: 0.4245 - val_loss: 2.1567\n",
      "Epoch 13/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 34ms/step - accuracy: 0.6664 - loss: 1.1344 - val_accuracy: 0.4218 - val_loss: 2.1697\n",
      "Epoch 14/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.6861 - loss: 1.0789 - val_accuracy: 0.4269 - val_loss: 2.1795\n",
      "Epoch 15/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.6838 - loss: 1.0665 - val_accuracy: 0.4255 - val_loss: 2.2008\n",
      "Epoch 16/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.6918 - loss: 1.0500 - val_accuracy: 0.4212 - val_loss: 2.2222\n",
      "Epoch 17/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.6989 - loss: 1.0293 - val_accuracy: 0.4105 - val_loss: 2.2487\n",
      "Epoch 18/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.7100 - loss: 0.9979 - val_accuracy: 0.4218 - val_loss: 2.2433\n",
      "Epoch 19/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.7130 - loss: 0.9834 - val_accuracy: 0.4148 - val_loss: 2.2889\n",
      "Epoch 20/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.7222 - loss: 0.9551 - val_accuracy: 0.4105 - val_loss: 2.3145\n",
      "[Stage 3] Val Loss (nats): 2.314\n",
      "[Stage 3] Val Loss (bits): 3.339\n",
      "[Stage 3] Val Perplexity:  10.119\n",
      "[Stage 3] Val Accuracy:    0.410\n",
      "\n",
      "[Stage 3] Sample text (prompt: 'MACBETH'):\n",
      "MACBETHugco. Nerawre vpon I haue af the Wat lesse of owen-hell: to drouning of Nigat:\n",
      "Who hil. Sleaues the King, welle great quell-same you\n",
      "\n",
      "   2. All hur ay sootule which with you it, call be me in to thick\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Stage 4] Training on 49940 samples (size=50000).\n",
      "Epoch 1/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 35ms/step - accuracy: 0.5901 - loss: 1.4979 - val_accuracy: 0.4563 - val_loss: 2.0353\n",
      "Epoch 2/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 36ms/step - accuracy: 0.6106 - loss: 1.3685 - val_accuracy: 0.4539 - val_loss: 2.0206\n",
      "Epoch 3/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 33ms/step - accuracy: 0.6219 - loss: 1.3304 - val_accuracy: 0.4533 - val_loss: 2.0062\n",
      "Epoch 4/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 32ms/step - accuracy: 0.6288 - loss: 1.2857 - val_accuracy: 0.4545 - val_loss: 1.9993\n",
      "Epoch 5/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 32ms/step - accuracy: 0.6376 - loss: 1.2464 - val_accuracy: 0.4612 - val_loss: 2.0013\n",
      "Epoch 6/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 32ms/step - accuracy: 0.6427 - loss: 1.2312 - val_accuracy: 0.4571 - val_loss: 2.0124\n",
      "Epoch 7/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.6469 - loss: 1.2148 - val_accuracy: 0.4638 - val_loss: 2.0181\n",
      "Epoch 8/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 32ms/step - accuracy: 0.6567 - loss: 1.1787 - val_accuracy: 0.4624 - val_loss: 2.0241\n",
      "Epoch 9/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 32ms/step - accuracy: 0.6629 - loss: 1.1648 - val_accuracy: 0.4579 - val_loss: 2.0406\n",
      "Epoch 10/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.6671 - loss: 1.1370 - val_accuracy: 0.4638 - val_loss: 2.0382\n",
      "Epoch 11/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 32ms/step - accuracy: 0.6751 - loss: 1.1193 - val_accuracy: 0.4541 - val_loss: 2.0504\n",
      "Epoch 12/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 32ms/step - accuracy: 0.6794 - loss: 1.0996 - val_accuracy: 0.4616 - val_loss: 2.0858\n",
      "Epoch 13/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.6860 - loss: 1.0813 - val_accuracy: 0.4638 - val_loss: 2.0797\n",
      "Epoch 14/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 32ms/step - accuracy: 0.6928 - loss: 1.0598 - val_accuracy: 0.4515 - val_loss: 2.0868\n",
      "Epoch 15/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.6992 - loss: 1.0333 - val_accuracy: 0.4519 - val_loss: 2.1257\n",
      "Epoch 16/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.7047 - loss: 1.0095 - val_accuracy: 0.4481 - val_loss: 2.1386\n",
      "Epoch 17/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.7092 - loss: 0.9987 - val_accuracy: 0.4555 - val_loss: 2.1471\n",
      "Epoch 18/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 32ms/step - accuracy: 0.7126 - loss: 0.9829 - val_accuracy: 0.4569 - val_loss: 2.1725\n",
      "Epoch 19/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 33ms/step - accuracy: 0.7194 - loss: 0.9667 - val_accuracy: 0.4557 - val_loss: 2.1837\n",
      "Epoch 20/20\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.7196 - loss: 0.9576 - val_accuracy: 0.4471 - val_loss: 2.2031\n",
      "[Stage 4] Val Loss (nats): 2.203\n",
      "[Stage 4] Val Loss (bits): 3.178\n",
      "[Stage 4] Val Perplexity:  9.053\n",
      "[Stage 4] Val Accuracy:    0.447\n",
      "\n",
      "[Stage 4] Sample text (prompt: 'MACBETH'):\n",
      "MACBETH[zMMPo. Ma. Ils diles call be deepe,\n",
      "And pleaues of the durther,\n",
      "And thinke vpon the Bell,\n",
      "me looker frumberlan? With the fell. With you?\n",
      "  Macb. My the Inder hapt as lass, to be will haue dong in sti\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# 1) Load Macbeth\n",
    "nltk.download('gutenberg', quiet=True)\n",
    "raw_text = gutenberg.raw('shakespeare-macbeth.txt')\n",
    "raw_text = raw_text[:50000]  # ~50k chars\n",
    "\n",
    "# 2) Hyperparameters\n",
    "partial_sizes = [10000, 20000, 30000, len(raw_text)]\n",
    "SEQ_LEN = 60        \n",
    "EMBED_DIM = 64      \n",
    "RNN_UNITS = 128     \n",
    "EPOCHS_PER_STAGE = 20  \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 3) Build a char-level LSTM model\n",
    "def build_char_model(vocab_size):\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(input_dim=vocab_size, \n",
    "                         output_dim=EMBED_DIM, \n",
    "                         input_length=SEQ_LEN),\n",
    "        layers.LSTM(RNN_UNITS, return_sequences=False),  # LSTM layer\n",
    "        layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 4) Utility functions\n",
    "def create_vocab(text):\n",
    "    chars = sorted(set(text))\n",
    "    char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx2char = {i: ch for ch, i in char2idx.items()}\n",
    "    return char2idx, idx2char\n",
    "\n",
    "def build_dataset(text, seq_len, char2idx):\n",
    "    X, y = [], []\n",
    "    for i in range(len(text) - seq_len):\n",
    "        seq = text[i : i + seq_len]\n",
    "        nxt = text[i + seq_len]\n",
    "        X.append([char2idx[ch] for ch in seq])\n",
    "        y.append(char2idx[nxt])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def sample_text(model, start_text, char2idx, idx2char, length=200):\n",
    "    \"\"\"Generate text from the model given a starting prompt.\"\"\"\n",
    "    text_out = start_text\n",
    "    for _ in range(length):\n",
    "        context = text_out[-SEQ_LEN:]\n",
    "        # pad if needed\n",
    "        if len(context) < SEQ_LEN:\n",
    "            context = ' ' * (SEQ_LEN - len(context)) + context\n",
    "        x_in = [char2idx.get(ch, 0) for ch in context]\n",
    "        x_in = np.array([x_in])\n",
    "        \n",
    "        preds = model.predict(x_in, verbose=0)[0]\n",
    "        next_idx = np.random.choice(len(preds), p=preds)\n",
    "        next_char = idx2char[next_idx]\n",
    "        text_out += next_char\n",
    "    return text_out\n",
    "\n",
    "# 5) Main logic\n",
    "char2idx_full, idx2char_full = create_vocab(raw_text)\n",
    "vocab_size = len(char2idx_full)\n",
    "model = build_char_model(vocab_size)\n",
    "\n",
    "for stage_idx, size in enumerate(partial_sizes, start=1):\n",
    "    partial_text = raw_text[:size]\n",
    "    X_data, y_data = build_dataset(partial_text, SEQ_LEN, char2idx_full)\n",
    "\n",
    "    # Train/Val split\n",
    "    split_i = int(0.9 * len(X_data))\n",
    "    X_train, X_val = X_data[:split_i], X_data[split_i:]\n",
    "    y_train, y_val = y_data[:split_i], y_data[split_i:]\n",
    "\n",
    "    if len(X_train) == 0:\n",
    "        print(f\"\\n[Stage {stage_idx}] Not enough data for size={size}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n[Stage {stage_idx}] Training on {len(X_data)} samples (size={size}).\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS_PER_STAGE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate perplexity\n",
    "    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "    val_loss_bits = val_loss / math.log(2)\n",
    "    val_pp = 2 ** val_loss_bits\n",
    "\n",
    "    print(f\"[Stage {stage_idx}] Val Loss (nats): {val_loss:.3f}\")\n",
    "    print(f\"[Stage {stage_idx}] Val Loss (bits): {val_loss_bits:.3f}\")\n",
    "    print(f\"[Stage {stage_idx}] Val Perplexity:  {val_pp:.3f}\")\n",
    "    print(f\"[Stage {stage_idx}] Val Accuracy:    {val_acc:.3f}\")\n",
    "\n",
    "    # Generate sample text\n",
    "    prompt = \"MACBETH\"\n",
    "    gen = sample_text(model, prompt, char2idx_full, idx2char_full, length=200)\n",
    "    print(f\"\\n[Stage {stage_idx}] Sample text (prompt: '{prompt}'):\\n{gen}\")\n",
    "    print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
