{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Models (GLMs)\n",
    "\n",
    "GLMs extend linear regression to handle non-Normal data (e.g., Binomial, Poisson).  \n",
    "Core components:  \n",
    "- **Linear Predictor**: $ \\eta_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_p x_{pi} $.  \n",
    "- **Link Function**: $ g(\\mu_i) = \\eta_i $.  \n",
    "- **Variance Function**: $ \\text{Var}(Y_i) = \\phi V(\\mu_i) $.  \n",
    "\n",
    "| **Distribution**     | **Link Function $g(\\mu)$**                                      | **Variance Function $V(\\mu)$**         | **Typical Use Case**                   |\n",
    "|-----------------------|---------------------------------------------------------------|----------------------------------------|----------------------------------------|\n",
    "| **Normal**            | $g(\\mu) = \\mu$ (Identity)                                     | $V(\\mu) = 1$                           | Continuous data with constant variance |\n",
    "| **Binomial**          | $g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right)$ (Logit)         | $V(\\mu) = \\mu(1 - \\mu)$               | Binary or proportion data              |\n",
    "|                       | $g(\\mu) = \\Phi^{-1}(\\mu)$ (Probit)                            | $V(\\mu) = \\mu(1 - \\mu)$               | Binary data, alternative link          |\n",
    "|                       | $g(\\mu) = \\log(-\\log(1-\\mu))$ (Complementary Log-Log)         | $V(\\mu) = \\mu(1 - \\mu)$               | Binary data, asymmetric relationships  |\n",
    "| **Poisson**           | $g(\\mu) = \\log(\\mu)$ (Log)                                    | $V(\\mu) = \\mu$                        | Count data                             |\n",
    "| **Gamma**             | $g(\\mu) = \\frac{1}{\\mu}$ (Inverse)                           | $V(\\mu) = \\mu^2$                      | Positive continuous data               |\n",
    "|                       | $g(\\mu) = \\log(\\mu)$ (Log)                                    | $V(\\mu) = \\mu^2$                      | Alternative link for skewed data       |\n",
    "| **Inverse Gaussian**  | $g(\\mu) = \\frac{1}{\\mu^2}$                                    | $V(\\mu) = \\mu^3$                      | Skewed positive continuous data         |\n",
    "\n",
    "## Notes:\n",
    "- The **canonical link function** for each distribution is highlighted in the table. Other link functions may be used, but the canonical link often has desirable statistical properties.\n",
    "- The variance functions reflect the relationship between the response mean and its variability, inherent to the exponential family distributions.\n",
    "- For custom scenarios, quasi-likelihood methods can allow more flexibility in specifying $V(\\mu)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  100\n",
      "Model:                            GLM   Df Residuals:                       98\n",
      "Model Family:                Gaussian   Df Model:                            1\n",
      "Link Function:               Identity   Scale:                          3.2922\n",
      "Method:                          IRLS   Log-Likelihood:                -200.46\n",
      "Date:                Sun, 12 Jan 2025   Deviance:                       322.63\n",
      "Time:                        18:37:51   Pearson chi2:                     323.\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):             0.9895\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.4704      0.182     13.547      0.000       2.113       2.828\n",
      "x1             0.6540      0.031     21.339      0.000       0.594       0.714\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "n_samples = 100\n",
    "x = np.random.uniform(-10, 10, n_samples)  # Predictor\n",
    "beta_0 = 2.5  # Intercept\n",
    "beta_1 = 0.7  # Slope\n",
    "y = beta_0 + beta_1 * x + np.random.normal(0, 2, n_samples)  # Response with noise\n",
    "\n",
    "# Add constant for the intercept\n",
    "X = sm.add_constant(x)\n",
    "\n",
    "# Fit a Gaussian GLM\n",
    "gaussian_model = sm.GLM(y, X, family=sm.families.Gaussian())\n",
    "gaussian_results = gaussian_model.fit()\n",
    "\n",
    "# Print the summary of results\n",
    "print(gaussian_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Negative Log-Likelihood = 100.0\n",
      "Epoch 1000: Negative Log-Likelihood = -128.48660278320312\n",
      "Epoch 2000: Negative Log-Likelihood = -128.48660278320312\n",
      "Epoch 3000: Negative Log-Likelihood = -128.48660278320312\n",
      "Epoch 4000: Negative Log-Likelihood = -128.48660278320312\n",
      "Epoch 5000: Negative Log-Likelihood = -128.48660278320312\n",
      "Epoch 6000: Negative Log-Likelihood = -128.48660278320312\n",
      "Epoch 7000: Negative Log-Likelihood = -128.48660278320312\n",
      "Epoch 8000: Negative Log-Likelihood = -128.48660278320312\n",
      "Epoch 9000: Negative Log-Likelihood = -128.48660278320312\n",
      "\n",
      "Comparison of Coefficients and Standard Errors:\n",
      "Coefficient      PT Coeff    PT StdErr    SM Coeff    SM StdErr\n",
      "-------------  ----------  -----------  ----------  -----------\n",
      "Beta_0           0.413635    0.0935555    0.413635    0.0935555\n",
      "Beta_1           1.05245     0.0706126    1.05245     0.0706126\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tabulate import tabulate\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 1. Generate synthetic Poisson data\n",
    "torch.manual_seed(42)\n",
    "n_samples = 100\n",
    "X = torch.cat([torch.ones(n_samples, 1), torch.randn(n_samples, 1)], dim=1)  # Add intercept\n",
    "beta_true = torch.tensor([0.5, 1.0])\n",
    "eta = X @ beta_true  # Linear predictor\n",
    "y = torch.poisson(torch.exp(eta))  # Poisson distributed response\n",
    "\n",
    "# Convert to numpy for statsmodels\n",
    "X_np = X.numpy()\n",
    "y_np = y.numpy()\n",
    "\n",
    "# 2. Define PyTorch Poisson GLM model\n",
    "class PoissonGLM(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(PoissonGLM, self).__init__()\n",
    "        self.beta = nn.Parameter(torch.zeros(n_features))  # Initialize coefficients\n",
    "\n",
    "    def forward(self, X):\n",
    "        eta = X @ self.beta  # Linear predictor\n",
    "        mu = torch.exp(eta)  # Mean (inverse of log link)\n",
    "        return mu\n",
    "\n",
    "    def log_likelihood(self, X, y):\n",
    "        mu = self.forward(X)\n",
    "        return torch.sum(y * torch.log(mu) - mu)  # Poisson log-likelihood\n",
    "\n",
    "# 3. Initialize PyTorch model and optimizer\n",
    "model = PoissonGLM(n_features=X.shape[1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 4. Train PyTorch model\n",
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    neg_log_likelihood = -model.log_likelihood(X, y)  # Negative log-likelihood\n",
    "    neg_log_likelihood.backward()  # Compute gradients\n",
    "    optimizer.step()  # Update parameters\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}: Negative Log-Likelihood = {neg_log_likelihood.item()}\")\n",
    "\n",
    "# 5. Compute standard errors using Hessian in PyTorch\n",
    "beta_hat = model.beta.detach()\n",
    "mu = model.forward(X).detach()\n",
    "hessian = torch.zeros((X.shape[1], X.shape[1]))\n",
    "for i in range(X.shape[0]):\n",
    "    x_i = X[i].view(-1, 1)\n",
    "    hessian += (x_i @ x_i.T) * mu[i]\n",
    "\n",
    "hessian_inv = torch.inverse(hessian)\n",
    "stderr = torch.sqrt(torch.diag(hessian_inv))  # Standard errors\n",
    "\n",
    "# Calculate PyTorch statistics\n",
    "t_values = beta_hat / stderr\n",
    "p_values = 2 * (1 - torch.distributions.Normal(0, 1).cdf(torch.abs(t_values)))\n",
    "confidence_intervals = torch.stack([beta_hat - 1.96 * stderr, beta_hat + 1.96 * stderr], dim=1)\n",
    "\n",
    "# 6. Statsmodels Poisson GLM\n",
    "sm_model = sm.GLM(y_np, X_np, family=sm.families.Poisson())\n",
    "sm_results = sm_model.fit()\n",
    "\n",
    "# 7. Create combined table for PyTorch and Statsmodels results\n",
    "# Create a table for comparison of coefficients and standard errors\n",
    "table = []\n",
    "for i in range(len(beta_hat)):\n",
    "    table.append([\n",
    "        f\"Beta_{i}\",  # Coefficient name\n",
    "        beta_hat[i].item(),  # PyTorch coefficient estimate\n",
    "        stderr[i].item(),  # PyTorch standard error\n",
    "        sm_results.params[i],  # Statsmodels coefficient estimate\n",
    "        sm_results.bse[i],  # Statsmodels standard error\n",
    "    ])\n",
    "\n",
    "# Print the comparison table\n",
    "print(\"\\nComparison of Coefficients and Standard Errors:\")\n",
    "print(tabulate(table, headers=[\"Coefficient\", \"PT Coeff\", \"PT StdErr\", \"SM Coeff\", \"SM StdErr\"]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
